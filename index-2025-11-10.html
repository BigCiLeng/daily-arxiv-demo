<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>arXiv Daily Digest</title>
  <style>
    :root {
      --bg-surface: #ffffff;
      --bg-app: #f4f6fb;
      --text-primary: #1f2933;
      --text-secondary: #4b5563;
      --brand: #2563eb;
      --brand-soft: rgba(37, 99, 235, 0.12);
      --border: #e2e8f0;
      --danger: #dc2626;
    }
    *, *::before, *::after {
      box-sizing: border-box;
    }
    body {
      font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
      margin: 0;
      background: var(--bg-app);
      color: var(--text-primary);
      line-height: 1.6;
      scroll-behavior: smooth;
    }
    header {
      background: radial-gradient(circle at top left, rgba(37, 99, 235, 0.82), #0f172a);
      color: white;
      padding: 36px 0 48px;
      box-shadow: 0 30px 60px rgba(15, 23, 42, 0.35);
    }
    header .inner {
      max-width: 1320px;
      margin: 0 auto;
      padding: 0 32px;
      display: flex;
      flex-direction: column;
      gap: 16px;
    }
    .page-title {
      font-size: clamp(2.2rem, 3vw, 2.8rem);
      font-weight: 700;
      margin: 0;
    }
    .page-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 18px;
      font-size: 0.95rem;
      color: rgba(248, 250, 252, 0.85);
    }
    .page-meta span::before {
      content: "â€¢";
      margin: 0 8px 0 4px;
      opacity: 0.5;
    }
    .page-meta span:first-child::before {
      content: "";
      margin: 0;
    }
    .source-switcher {
      display: inline-flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-top: 6px;
    }
    .source-button {
      appearance: none;
      border: 1px solid rgba(148, 163, 184, 0.35);
      background: rgba(15, 23, 42, 0.2);
      color: white;
      padding: 6px 16px;
      border-radius: 999px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, transform 0.2s ease, border-color 0.2s ease;
    }
    .source-button:hover,
    .source-button:focus {
      background: rgba(37, 99, 235, 0.35);
      border-color: rgba(96, 165, 250, 0.75);
      outline: none;
    }
    .source-button.is-active {
      background: white;
      color: var(--brand);
      border-color: transparent;
      box-shadow: 0 6px 18px rgba(14, 116, 244, 0.35);
      transform: translateY(-1px);
    }
    .display-mode {
      display: inline-flex;
      flex-wrap: wrap;
      gap: 8px;
      align-items: center;
      margin-top: 12px;
    }
    .display-mode__label {
      font-size: 0.9rem;
      font-weight: 600;
      color: rgba(248, 250, 252, 0.85);
      margin-right: 4px;
    }
    .display-mode__button {
      appearance: none;
      border: 1px solid rgba(148, 163, 184, 0.35);
      background: rgba(15, 23, 42, 0.2);
      color: white;
      padding: 6px 14px;
      border-radius: 999px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, transform 0.2s ease, border-color 0.2s ease;
    }
    .display-mode__button:hover,
    .display-mode__button:focus {
      background: rgba(37, 99, 235, 0.35);
      border-color: rgba(96, 165, 250, 0.75);
      outline: none;
    }
    .display-mode__button.is-active {
      background: white;
      color: var(--brand);
      border-color: transparent;
      box-shadow: 0 6px 18px rgba(14, 116, 244, 0.35);
      transform: translateY(-1px);
    }
    .layout {
      display: flex;
      gap: 32px;
      align-items: flex-start;
      max-width: 1320px;
      margin: -32px auto 48px;
      padding: 0 32px 64px;
    }
    .sidebar {
      position: sticky;
      top: 24px;
      flex: 0 0 300px;
      background: var(--bg-surface);
      border-radius: 20px;
      padding: 24px 20px;
      box-shadow: 0 24px 48px rgba(15, 23, 42, 0.12);
      border: 1px solid var(--border);
      display: flex;
      flex-direction: column;
      gap: 18px;
      height: fit-content;
      max-height: calc(100vh - 48px);
    }
    .preferences-card {
      background: linear-gradient(140deg, rgba(37, 99, 235, 0.08), rgba(2, 132, 199, 0.1));
      border-radius: 16px;
      padding: 18px 20px;
      border: 1px solid rgba(15, 23, 42, 0.08);
      display: flex;
      flex-direction: column;
      gap: 14px;
    }
    .preferences-card h2 {
      margin: 0;
      font-size: 1rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--text-secondary);
    }
    .preferences-view {
      display: flex;
      flex-direction: column;
      gap: 12px;
    }
    .preferences-group {
      display: flex;
      flex-direction: column;
      gap: 6px;
    }
    .preferences-label {
      font-size: 0.85rem;
      font-weight: 600;
      color: var(--text-secondary);
    }
    .preferences-empty {
      font-size: 0.9rem;
      color: var(--text-secondary);
    }
    .preferences-edit {
      align-self: flex-start;
      padding: 6px 16px;
      border-radius: 10px;
      border: none;
      background: var(--brand);
      color: white;
      font-weight: 600;
      cursor: pointer;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .preferences-edit:hover,
    .preferences-edit:focus {
      transform: translateY(-1px);
      box-shadow: 0 8px 20px rgba(37, 99, 235, 0.25);
      outline: none;
    }
    .preferences-card textarea {
      width: 100%;
      min-height: 70px;
      resize: vertical;
      border-radius: 10px;
      border: 1px solid rgba(148, 163, 184, 0.4);
      padding: 10px 12px;
      font-size: 0.9rem;
      font-family: inherit;
      background: rgba(255, 255, 255, 0.82);
      color: var(--text-primary);
    }
    .preferences-actions {
      display: flex;
      gap: 10px;
      margin-top: 6px;
      flex-wrap: wrap;
    }
    .preferences-actions button {
      flex: 1 1 0;
      padding: 8px 12px;
      border-radius: 10px;
      border: none;
      font-weight: 600;
      cursor: pointer;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .preferences-actions button[type="submit"] {
      background: var(--brand);
      color: white;
      box-shadow: 0 10px 24px rgba(37, 99, 235, 0.32);
    }
    .preferences-actions button[type="submit"]:hover,
    .preferences-actions button[type="submit"]:focus {
      transform: translateY(-1px);
      outline: none;
    }
    .preferences-actions button.preferences-cancel,
    .preferences-actions button.preferences-reset {
      background: rgba(15, 23, 42, 0.08);
      color: var(--text-secondary);
    }
    .preferences-actions button.preferences-cancel:hover,
    .preferences-actions button.preferences-cancel:focus,
    .preferences-actions button.preferences-reset:hover,
    .preferences-actions button.preferences-reset:focus {
      background: rgba(15, 23, 42, 0.15);
      outline: none;
    }
    .preferences-status {
      min-height: 20px;
      font-size: 0.85rem;
      color: var(--text-secondary);
    }
    .nav-title {
      font-size: 0.9rem;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--text-secondary);
      margin-top: 4px;
    }
    .sidebar nav {
      overflow-y: auto;
      padding-right: 6px;
      flex: 1;
    }
    .sidebar nav::-webkit-scrollbar {
      width: 6px;
    }
    .sidebar nav::-webkit-scrollbar-thumb {
      background: rgba(148, 163, 184, 0.5);
      border-radius: 999px;
    }
    .nav-list {
      list-style: none;
      padding-left: 0;
      margin: 0;
      display: flex;
      flex-direction: column;
      gap: 6px;
    }
    .nav-list.nav-level-2 {
      padding-left: 18px;
      gap: 4px;
    }
    .nav-list.nav-level-3 {
      padding-left: 18px;
      gap: 4px;
    }
    .nav-item a {
      display: block;
      padding: 8px 10px;
      border-radius: 10px;
      color: var(--text-secondary);
      text-decoration: none;
      transition: background 0.2s ease, color 0.2s ease;
    }
    .nav-item a:hover,
    .nav-item a:focus {
      background: var(--brand-soft);
      color: var(--brand);
      outline: none;
    }
    .nav-item a.is-active {
      background: var(--brand);
      color: white;
    }
    .content {
      flex: 1;
      min-width: 0;
    }
    .content-section {
      background: var(--bg-surface);
      border-radius: 24px;
      padding: 28px 32px;
      box-shadow: 0 24px 48px rgba(15, 23, 42, 0.08);
      border: 1px solid var(--border);
      margin-bottom: 36px;
      transition: box-shadow 0.25s ease, transform 0.25s ease;
      position: relative;
    }
    .content-section.is-hidden {
      display: none;
    }
    .content-section:not(.is-collapsed):hover {
      box-shadow: 0 28px 60px rgba(15, 23, 42, 0.12);
      transform: translateY(-2px);
    }
    .section-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 16px;
      margin-bottom: 20px;
      flex-wrap: wrap;
    }
    .section-header h2 {
      margin: 0;
      font-size: 1.6rem;
    }
    .section-summary {
      margin: 0;
      font-size: 0.95rem;
      color: var(--text-secondary);
    }
    .section-toggle {
      appearance: none;
      border: none;
      background: var(--brand-soft);
      color: var(--brand);
      border-radius: 999px;
      font-weight: 600;
      padding: 6px 18px;
      cursor: pointer;
      transition: background 0.2s ease, transform 0.2s ease;
    }
    .section-toggle:hover,
    .section-toggle:focus {
      background: rgba(37, 99, 235, 0.2);
      outline: none;
      transform: translateY(-1px);
    }
    .section-body {
      display: flex;
      flex-direction: column;
      gap: 24px;
    }
    .content-section.is-collapsed .section-body {
      display: none;
    }
    .stats-grid {
      display: grid;
      gap: 20px;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
    }
    .stat-card {
      background: linear-gradient(145deg, rgba(37, 99, 235, 0.08), rgba(2, 132, 199, 0.08));
      border-radius: 18px;
      padding: 20px;
      border: 1px solid rgba(15, 23, 42, 0.08);
      display: flex;
      flex-direction: column;
      gap: 12px;
    }
    .stat-card h3 {
      font-size: 1rem;
      margin: 0;
      color: var(--text-secondary);
      text-transform: uppercase;
      letter-spacing: 0.08em;
    }
    .stat-card p {
      margin: 0;
      font-size: 1.1rem;
      font-weight: 600;
    }
    .stat-card ul {
      margin: 0;
      padding-left: 18px;
      color: var(--text-secondary);
      font-size: 0.95rem;
      display: flex;
      flex-direction: column;
      gap: 4px;
    }
    .paper {
      background: var(--bg-surface);
      border-radius: 18px;
      padding: 22px 24px;
      border: 1px solid var(--border);
      box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.6);
      cursor: pointer;
      transition: border-color 0.2s ease, box-shadow 0.2s ease, transform 0.2s ease;
    }
    .paper + .paper {
      margin-top: 18px;
    }
    .paper h3 {
      margin: 0 0 12px 0;
      font-size: 1.2rem;
      display: flex;
      align-items: center;
      gap: 12px;
      flex-wrap: wrap;
    }
    .paper h3 .quick-view-button {
      flex: 0 0 auto;
    }
    .keyword-tags {
      display: inline-flex;
      gap: 6px;
      flex-wrap: wrap;
      margin: 0 4px;
    }
    .keyword-tag {
      background: rgba(15, 23, 42, 0.08);
      color: var(--text-secondary);
      font-size: 0.75rem;
      letter-spacing: 0.02em;
      padding: 4px 8px;
      border-radius: 999px;
      font-weight: 600;
      text-transform: uppercase;
    }
    .paper h3 a {
      color: inherit;
      text-decoration: none;
    }
    .paper h3 a:hover {
      color: var(--brand);
    }
    .paper .meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.95rem;
      color: var(--text-secondary);
      margin-bottom: 10px;
    }
    .paper .summary {
      font-size: 0.95rem;
      color: var(--text-primary);
      margin: 2px 0 10px;
    }
    .paper .meta .id {
      font-weight: 600;
      color: var(--brand);
    }
    .paper .subjects {
      font-size: 0.95rem;
      color: var(--text-secondary);
      margin-bottom: 12px;
    }
    .paper .abstract {
      color: var(--text-primary);
      margin-bottom: 16px;
    }
    .paper .links {
      display: flex;
      gap: 16px;
      font-weight: 600;
      flex-wrap: wrap;
      margin: 0;
    }
    .paper .links a {
      color: var(--brand);
      text-decoration: none;
    }
    .paper .links a:hover {
      text-decoration: underline;
    }
    .paper .link-button {
      appearance: none;
      border: 1px solid rgba(37, 99, 235, 0.4);
      background: rgba(37, 99, 235, 0.08);
      color: var(--brand);
      padding: 6px 14px;
      border-radius: 999px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, transform 0.2s ease, border-color 0.2s ease;
    }
    .paper .link-button:hover,
    .paper .link-button:focus {
      background: rgba(37, 99, 235, 0.18);
      border-color: rgba(37, 99, 235, 0.55);
      outline: none;
      transform: translateY(-1px);
    }
    body.display-mode-title .paper:not(.paper--expanded) .quick-view-button,
    body.display-mode-authors .paper:not(.paper--expanded) .quick-view-button {
      padding: 4px 10px;
      font-size: 0.85rem;
    }
    .paper:hover {
      box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.6), 0 12px 24px rgba(15, 23, 42, 0.12);
      transform: translateY(-1px);
    }
    .paper.paper--expanded {
      border-color: var(--brand);
      box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.6), 0 16px 32px rgba(37, 99, 235, 0.18);
    }
    body.display-mode-title .paper:not(.paper--expanded) .meta,
    body.display-mode-title .paper:not(.paper--expanded) .summary,
    body.display-mode-title .paper:not(.paper--expanded) .subjects,
    body.display-mode-title .paper:not(.paper--expanded) .abstract {
      display: none;
    }
    body.display-mode-authors .paper:not(.paper--expanded) .meta .id {
      display: none;
    }
    body.display-mode-authors .paper:not(.paper--expanded) .subjects,
    body.display-mode-authors .paper:not(.paper--expanded) .abstract {
      display: none;
    }
    body.display-mode-title .paper:not(.paper--expanded),
    body.display-mode-authors .paper:not(.paper--expanded) {
      padding: 12px 16px;
      border-radius: 12px;
      border-color: rgba(148, 163, 184, 0.6);
      box-shadow: none;
      background: rgba(255, 255, 255, 0.92);
      display: grid;
      grid-template-columns: 1fr;
      row-gap: 4px;
    }
    body.display-mode-title .paper:not(.paper--expanded) h3,
    body.display-mode-authors .paper:not(.paper--expanded) h3 {
      margin: 0;
      font-size: 1rem;
      line-height: 1.3;
    }
    body.display-mode-title .paper:not(.paper--expanded) h3 a,
    body.display-mode-authors .paper:not(.paper--expanded) h3 a {
      display: inline-block;
      max-width: 100%;
    }
    body.display-mode-authors .paper:not(.paper--expanded) .meta {
      margin-bottom: 0;
      gap: 6px;
      font-size: 0.85rem;
      line-height: 1.3;
    }
    body.display-mode-authors .paper:not(.paper--expanded) .authors {
      font-weight: 500;
      color: var(--text-secondary);
    }
    body.display-mode-title .paper:not(.paper--expanded) + .paper:not(.paper--expanded),
    body.display-mode-authors .paper:not(.paper--expanded) + .paper:not(.paper--expanded) {
      margin-top: 8px;
    }
    body.display-mode-title .paper:not(.paper--expanded) .links,
    body.display-mode-authors .paper:not(.paper--expanded) .links {
      gap: 8px;
    }
    body.display-mode-title .paper:not(.paper--expanded) .links a,
    body.display-mode-authors .paper:not(.paper--expanded) .links a {
      display: none;
    }
    body.display-mode-title .paper:not(.paper--expanded) .links .link-button,
    body.display-mode-authors .paper:not(.paper--expanded) .links .link-button {
      padding: 4px 10px;
      font-size: 0.85rem;
    }
    body.display-mode-title .paper:not(.paper--expanded) .keyword-tag,
    body.display-mode-authors .paper:not(.paper--expanded) .keyword-tag {
      font-size: 0.7rem;
      padding: 3px 6px;
    }
    body.modal-open {
      overflow: hidden;
    }
    .abstract-modal {
      position: fixed;
      inset: 0;
      display: none;
      align-items: center;
      justify-content: center;
      z-index: 1200;
    }
    .abstract-modal.is-open {
      display: flex;
    }
    .abstract-modal__backdrop {
      position: absolute;
      inset: 0;
      background: rgba(15, 23, 42, 0.55);
    }
    .abstract-modal__dialog {
      position: relative;
      width: min(960px, 92vw);
      height: min(80vh, 720px);
      background: white;
      border-radius: 18px;
      box-shadow: 0 40px 80px rgba(15, 23, 42, 0.35);
      border: 1px solid rgba(148, 163, 184, 0.35);
      display: flex;
      flex-direction: column;
      overflow: hidden;
      z-index: 1;
    }
    .abstract-modal__header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 16px 20px;
      border-bottom: 1px solid rgba(226, 232, 240, 0.8);
      gap: 12px;
    }
    .abstract-modal__title {
      margin: 0;
      font-size: 1.1rem;
      font-weight: 600;
      line-height: 1.4;
      color: var(--text-primary);
    }
    .abstract-modal__close {
      appearance: none;
      border: none;
      background: rgba(15, 23, 42, 0.04);
      color: var(--text-secondary);
      padding: 6px 10px;
      border-radius: 10px;
      cursor: pointer;
      font-weight: 600;
      transition: background 0.2s ease, color 0.2s ease;
    }
    .abstract-modal__close:hover,
    .abstract-modal__close:focus {
      background: rgba(37, 99, 235, 0.15);
      color: var(--brand);
      outline: none;
    }
    .abstract-modal__body {
      flex: 1;
      padding: 20px 24px;
      display: flex;
      flex-direction: column;
      gap: 18px;
      overflow-y: auto;
      background: linear-gradient(135deg, rgba(248, 250, 252, 0.9), rgba(241, 245, 249, 0.6));
    }
    .abstract-modal__meta {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      font-size: 0.92rem;
      color: var(--text-secondary);
    }
    .abstract-modal__id {
      font-weight: 600;
      color: var(--brand);
    }
    .abstract-modal__authors {
      flex: 1 1 100%;
    }
    .abstract-modal__subjects {
      flex: 1 1 100%;
    }
    .abstract-modal__summary {
      flex: 1 1 100%;
      color: var(--text-primary);
    }
    .abstract-modal__abstract {
      font-size: 0.95rem;
      color: var(--text-primary);
      line-height: 1.65;
      white-space: pre-line;
    }
    .abstract-modal__actions {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-top: auto;
    }
    .abstract-modal__actions a {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 8px 16px;
      border-radius: 999px;
      font-weight: 600;
      text-decoration: none;
      transition: background 0.2s ease, color 0.2s ease, transform 0.2s ease, border-color 0.2s ease;
    }
    .abstract-modal__actions a.primary {
      background: var(--brand);
      color: white;
      box-shadow: 0 10px 24px rgba(37, 99, 235, 0.28);
    }
    .abstract-modal__actions a.primary:hover,
    .abstract-modal__actions a.primary:focus {
      background: #1d4ed8;
      transform: translateY(-1px);
      outline: none;
    }
    .abstract-modal__actions a.secondary {
      border: 1px solid rgba(37, 99, 235, 0.35);
      color: var(--brand);
      background: rgba(37, 99, 235, 0.1);
    }
    .abstract-modal__actions a.secondary:hover,
    .abstract-modal__actions a.secondary:focus {
      background: rgba(37, 99, 235, 0.18);
      outline: none;
      transform: translateY(-1px);
    }
    .empty-state {
      background: var(--bg-surface);
      border: 1px dashed var(--border);
      border-radius: 14px;
      padding: 18px 20px;
      color: var(--text-secondary);
    }
    .chip-set {
      display: flex;
      gap: 8px;
      flex-wrap: wrap;
      margin-top: 2px;
    }
    .chip {
      display: inline-flex;
      align-items: center;
      padding: 4px 12px;
      border-radius: 999px;
      background: var(--brand-soft);
      color: var(--brand);
      font-size: 0.85rem;
      font-weight: 600;
    }
    .watcher-summary {
      display: flex;
      flex-direction: column;
      gap: 6px;
      font-size: 0.93rem;
      color: var(--text-secondary);
    }
    .subject-grid {
      display: grid;
      gap: 28px;
      grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
    }
    .category-block {
      padding: 12px 0 4px;
      border-top: 1px solid rgba(148, 163, 184, 0.3);
    }
    .category-block:first-of-type {
      border-top: none;
      padding-top: 0;
    }
    .category-block__header {
      display: flex;
      align-items: baseline;
      gap: 12px;
      margin-bottom: 18px;
    }
    .category-block__header h3 {
      margin: 0;
      font-size: 1.35rem;
    }
    .subject-group {
      display: flex;
      flex-direction: column;
      gap: 14px;
      padding: 18px;
      border-radius: 16px;
      border: 1px solid rgba(148, 163, 184, 0.2);
      background: linear-gradient(145deg, rgba(255, 255, 255, 0.95), rgba(241, 245, 249, 0.6));
    }
    .subject-group__header {
      display: flex;
      align-items: baseline;
      gap: 12px;
    }
    .subject-group__header h4 {
      margin: 0;
      font-size: 1.05rem;
    }
    .count-chip {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 2px 10px;
      border-radius: 999px;
      background: var(--brand-soft);
      color: var(--brand);
      font-size: 0.85rem;
      font-weight: 600;
    }
    .skip-link {
      position: absolute;
      top: -48px;
      left: 16px;
      background: var(--brand);
      color: white;
      padding: 10px 16px;
      border-radius: 8px;
      text-decoration: none;
      font-weight: 600;
      transition: top 0.2s ease;
      z-index: 10;
    }
    .skip-link:focus {
      top: 16px;
    }
    footer {
      background: #0f172a;
      color: rgba(248, 250, 252, 0.9);
      padding: 24px 32px;
      font-size: 0.9rem;
      text-align: center;
    }
    footer a {
      color: #93c5fd;
      text-decoration: none;
    }
    footer a:hover {
      text-decoration: underline;
    }
    .noscript-warning {
      max-width: 960px;
      margin: 32px auto;
      padding: 20px;
      background: rgba(254, 240, 138, 0.35);
      border: 1px solid rgba(250, 204, 21, 0.6);
      border-radius: 12px;
      color: #92400e;
      font-size: 0.95rem;
    }
    @media (max-width: 960px) {
      .layout {
        flex-direction: column;
        padding: 0 20px 40px;
        gap: 24px;
      }
      .sidebar {
        position: static;
        width: 100%;
        max-height: none;
      }
      .content {
        width: 100%;
      }
      .nav-list {
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
      }
      .nav-list.nav-level-2,
      .nav-list.nav-level-3 {
        width: 100%;
        padding-left: 0;
      }
      .nav-list.nav-level-2 .nav-item a,
      .nav-list.nav-level-3 .nav-item a {
        padding-left: 14px;
      }
    }
    @media (max-width: 640px) {
      header .inner {
        padding: 0 20px;
      }
      .content-section {
        padding: 22px;
      }
      .stats-grid {
        grid-template-columns: 1fr;
      }
      .subject-grid {
        grid-template-columns: 1fr;
      }
      .layout {
        margin-top: -24px;
      }
      .preferences-actions {
        flex-direction: column;
      }
      .preferences-actions button {
        flex: 1 1 auto;
      }
      .source-switcher {
        justify-content: flex-start;
      }
      .display-mode {
        justify-content: flex-start;
      }
    }
  </style>
</head>
<body class="display-mode-authors">
  <a class="skip-link" href="#main-content">Skip to content</a>
  <header>
    <div class="inner">
      <h1 class="page-title">arXiv cs Daily Digest</h1>
      <div class="page-meta">
        <span id="meta-source">Source: Computer Vision (cs.CV)</span>
        <span id="meta-date">Date: 2025-11-10</span>
        <span id="meta-generated">Generated at: 2025-11-10 04:34 UTC</span>
        <span id="meta-total">Total papers: 79</span>
      </div>
      <div class="source-switcher" id="source-switcher" role="group" aria-label="Select arXiv category"></div>
      <div class="display-mode" id="display-mode-controls" role="group" aria-label="Select paper layout"></div>
    </div>
  </header>
  <noscript>
    <div class="noscript-warning">This dashboard requires JavaScript to filter sources and update preferences. Please enable JavaScript in your browser.</div>
  </noscript>
  <div class="layout">
    <aside class="sidebar">
      <div class="preferences-card">
        <h2>Tracking</h2>
        <div id="preferences-view" class="preferences-view">
          <div class="preferences-group">
            <span class="preferences-label">Favorite authors</span>
            <div class="chip-set" id="favorite-authors-view"></div>
          </div>
          <div class="preferences-group">
            <span class="preferences-label">Watched keywords</span>
            <div class="chip-set" id="keywords-view"></div>
          </div>
          <button type="button" id="edit-preferences" class="preferences-edit">Edit</button>
          <p class="preferences-status" id="preferences-status-view" role="status" aria-live="polite"></p>
        </div>
        <form id="preferences-form" hidden>
          <label for="favorite-authors-input" class="preferences-label">Favorite authors</label>
          <textarea id="favorite-authors-input" placeholder="One per line or comma separated">barron
Fei-Fei Li</textarea>
          <label for="keywords-input" class="preferences-label">Watched keywords</label>
          <textarea id="keywords-input" placeholder="One per line or comma separated">reconstruction
gaussian splatting</textarea>
          <div class="preferences-actions">
            <button type="submit">Save</button>
            <button type="button" id="cancel-preferences" class="preferences-cancel">Cancel</button>
            <button type="button" id="reset-preferences" class="preferences-reset">Reset</button>
          </div>
          <p class="preferences-status" id="preferences-status" role="status" aria-live="polite"></p>
        </form>
      </div>
      <div class="nav-title">On this page</div>
      <nav aria-label="Section navigation"></nav>
    </aside>
    <main id="main-content" class="content">
      <section id="overview" class="content-section is-collapsed is-hidden">
        <div class="section-header">
          <h2>All Papers</h2>
          <p class="section-summary" id="overview-summary"></p>
        </div>
        <div class="section-body" id="overview-body"></div>
      </section>
      <section id="stats" class="content-section" data-collapsible="true">
        <div class="section-header">
          <h2>Statistics</h2>
          <button type="button" class="section-toggle" data-target="stats" aria-expanded="true">Hide section</button>
        </div>
        <div class="section-body" id="stats-body"></div>
      </section>
      <section id="favorite" class="content-section is-collapsed is-hidden" data-collapsible="true">
        <div class="section-header">
          <h2>Favorite Authors</h2>
          <button type="button" class="section-toggle" data-target="favorite" aria-expanded="false">Show section</button>
        </div>
        <div class="section-body" id="favorite-body"></div>
      </section>
      <section id="keyword" class="content-section is-collapsed is-hidden" data-collapsible="true">
        <div class="section-header">
          <h2>Watched Keywords</h2>
          <button type="button" class="section-toggle" data-target="keyword" aria-expanded="false">Show section</button>
        </div>
        <div class="section-body" id="keywords-body"></div>
      </section>
      <section id="categories" class="content-section is-collapsed is-hidden" data-collapsible="true">
        <div class="section-header">
          <h2>Browse by Category</h2>
          <button type="button" class="section-toggle" data-target="categories" aria-expanded="false">Show section</button>
        </div>
        <div class="section-body" id="categories-body"></div>
      </section>
    </main>
  </div>
  <div class="abstract-modal" id="abstract-modal" aria-hidden="true">
    <div class="abstract-modal__backdrop" data-modal-dismiss="true"></div>
    <div class="abstract-modal__dialog" role="dialog" aria-modal="true" aria-labelledby="abstract-modal-title">
      <div class="abstract-modal__header">
        <h2 class="abstract-modal__title" id="abstract-modal-title">Preview abstract</h2>
        <button type="button" class="abstract-modal__close" id="abstract-modal-close">Close</button>
      </div>
      <div class="abstract-modal__body" id="abstract-modal-body">
        <div class="abstract-modal__meta">
          <span class="abstract-modal__id" id="abstract-modal-id"></span>
          <span class="abstract-modal__authors" id="abstract-modal-authors"></span>
          <span class="abstract-modal__subjects" id="abstract-modal-subjects"></span>
          <span class="abstract-modal__summary" id="abstract-modal-summary" hidden></span>
        </div>
        <div class="abstract-modal__abstract" id="abstract-modal-abstract"></div>
        <div class="abstract-modal__actions">
          <a href="#" target="_blank" rel="noopener" class="primary" id="abstract-modal-original">Open on arXiv</a>
          <a href="#" target="_blank" rel="noopener" class="secondary" id="abstract-modal-pdf" hidden>Download PDF</a>
        </div>
      </div>
    </div>
  </div>
  <footer>
    Source: <a id="footer-source" href="https://arxiv.org/list/cs.CV/recent?skip=0&amp;show=2000" target="_blank" rel="noopener">Computer Vision (cs.CV)</a>
  </footer>
  <script type="application/json" id="digest-data">{"generated_at": "2025-11-10 04:34 UTC", "sources": {"cs.CV": {"label": "Computer Vision (cs.CV)", "url": "https://arxiv.org/list/cs.CV/recent?skip=0&show=2000", "date": "2025-11-10", "articles": [{"arxiv_id": "arXiv:2511.05491", "title": "Visual Spatial Tuning", "abs_url": "https://arxiv.org/abs/2511.05491", "pdf_url": "https://arxiv.org/pdf/2511.05491", "authors": ["Rui Yang", "Ziyu Zhu", "Yanwei Li", "Jingjia Huang", "Shen Yan", "Siyuan Zhou", "Zhe Liu", "Xiangtai Li", "Shuangye Li", "Wenqian Wang", "Yi Lin", "Hengshuang Zhao"], "abstract": "Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8\\%$ on MMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Visual Spatial Tuning", "spatial perception"], "summary": "The paper introduces VST, a framework to enhance VLMs' spatial abilities through datasets and training methods, improving performance on spatial benchmarks."}, {"arxiv_id": "arXiv:2511.05489", "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning", "abs_url": "https://arxiv.org/abs/2511.05489", "pdf_url": "https://arxiv.org/pdf/2511.05489", "authors": ["Junwen Pan", "Qizhe Zhang", "Rui Zhang", "Ming Lu", "Xin Wan", "Yuan Zhang", "Chang Liu", "Qi She"], "abstract": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Temporal search", "Reinforcement learning"], "summary": "TimeSearch-R uses reinforcement learning to optimize temporal search by integrating video reasoning and ensuring complete exploration of video content."}, {"arxiv_id": "arXiv:2511.05477", "title": "GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation", "abs_url": "https://arxiv.org/abs/2511.05477", "pdf_url": "https://arxiv.org/pdf/2511.05477", "authors": ["Guojie Li", "Anwar P.P. Abdul Majeed", "Muhammad Ateeq", "Anh Nguyen", "Fan Zhang"], "abstract": "Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["GroupKAN", "Kolmogorov-Arnold Networks"], "summary": "GroupKAN introduces Grouped KAN Transform and Activation for efficient, interpretable medical image segmentation, outperforming U-KAN on IoU and parameter efficiency."}, {"arxiv_id": "arXiv:2511.05474", "title": "Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection", "abs_url": "https://arxiv.org/abs/2511.05474", "pdf_url": "https://arxiv.org/pdf/2511.05474", "authors": ["Xian-Hong Huang", "Hui-Kai Su", "Chi-Chia Sun", "Jun-Wei Hsieh"], "abstract": "This paper introduces a cutting-edge approach to cross-modal interaction for tiny object detection by combining semantic-guided natural language processing with advanced visual recognition backbones. The proposed method integrates the BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By employing lemmatization and fine-tuning techniques, the system aligns semantic cues from textual inputs with visual features, enhancing detection precision for small and complex objects. Experimental validation using the COCO and Objects365 datasets demonstrates that the model achieves superior performance. On the COCO2017 validation set, it attains a 52.6% average precision (AP), outperforming YOLO-World significantly while maintaining half the parameter consumption of Transformer-based models like GLIP. Several test on different of backbones such ELAN, MSP, and CSP further enable efficient handling of multi-scale objects, ensuring scalability and robustness in resource-constrained environments. This study underscores the potential of integrating natural language understanding with advanced backbone architectures, setting new benchmarks in object detection accuracy, efficiency, and adaptability to real-world challenges.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["cross-modal interaction", "tiny object detection"], "summary": "This paper presents a method combining BERT and PRB-FPN-Net for enhancing tiny object detection accuracy and efficiency."}, {"arxiv_id": "arXiv:2511.05467", "title": "EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes", "abs_url": "https://arxiv.org/abs/2511.05467", "pdf_url": "https://arxiv.org/pdf/2511.05467", "authors": ["Sanghyeon Chang", "Srikar Arani", "Nishant Sai Nuthalapati", "Youngjoon Suh", "Nicholas Choi", "Siavash Khodakarami", "Md Rakibul Hasan Roni", "Nenad Miljkovic", "Aparna Chandramowlishwaran", "Yoonjin Won"], "abstract": "Flow boiling is an efficient heat transfer mechanism capable of dissipating high heat loads with minimal temperature variation, making it an ideal thermal management method. However, sudden shifts between flow regimes can disrupt thermal performance and system reliability, highlighting the need for accurate and low-latency real-time monitoring. Conventional optical imaging methods are limited by high computational demands and insufficient temporal resolution, making them inadequate for capturing transient flow behavior. To address this, we propose a real-time framework based on signals from neuromorphic sensors for flow regime classification. Neuromorphic sensors detect changes in brightness at individual pixels, which typically correspond to motion at edges, enabling fast and efficient detection without full-frame reconstruction, providing event-based information. We develop five classification models using both traditional image data and event-based data, demonstrating that models leveraging event data outperform frame-based approaches due to their sensitivity to dynamic flow features. Among these models, the event-based long short-term memory model provides the best balance between accuracy and speed, achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our asynchronous processing pipeline supports continuous, low-latency predictions and delivers stable output through a majority voting mechanisms, enabling reliable real-time feedback for experimental control and intelligent thermal management.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["neuromorphic sensors", "flow regime classification"], "summary": "This paper proposes a real-time flow regime classification framework using neuromorphic sensors, outperforming traditional methods with higher accuracy and lower latency."}, {"arxiv_id": "arXiv:2511.05464", "title": "Photo Dating by Facial Age Aggregation", "abs_url": "https://arxiv.org/abs/2511.05464", "pdf_url": "https://arxiv.org/pdf/2511.05464", "authors": ["Jakub Paplham", "Vojtech Franc"], "abstract": "We introduce a novel method for Photo Dating which estimates the year a photograph was taken by leveraging information from the faces of people present in the image. To facilitate this research, we publicly release CSFD-1.6M, a new dataset containing over 1.6 million annotated faces, primarily from movie stills, with identity and birth year annotations. Uniquely, our dataset provides annotations for multiple individuals within a single image, enabling the study of multi-face information aggregation. We propose a probabilistic framework that formally combines visual evidence from modern face recognition and age estimation models, and career-based temporal priors to infer the photo capture year. Our experiments demonstrate that aggregating evidence from multiple faces consistently improves the performance and the approach significantly outperforms strong, scene-based baselines, particularly for images containing several identifiable individuals.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Photo Dating", "multi-face information"], "summary": "The paper introduces a novel method for estimating photo capture years using facial information and a new dataset with multiple annotated faces."}, {"arxiv_id": "arXiv:2511.05461", "title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2", "abs_url": "https://arxiv.org/abs/2511.05461", "pdf_url": "https://arxiv.org/pdf/2511.05461", "authors": ["Olivier Dietrich", "Merlin Alfredsson", "Emilia Arens", "Nando Metzger", "Torben Peters", "Linus Scheibenreif", "Jan Dirk Wegner", "Konrad Schindler"], "abstract": "Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$\\,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Copernicus images", "building damage assessment"], "summary": "This paper shows that medium-resolution Copernicus images can effectively assess building damage after natural disasters, supporting rapid humanitarian response."}, {"arxiv_id": "arXiv:2511.05449", "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?", "abs_url": "https://arxiv.org/abs/2511.05449", "pdf_url": "https://arxiv.org/pdf/2511.05449", "authors": ["Tuan Anh Tran", "Duy M. H. Nguyen", "Hoai-Chau Tran", "Michael Barz", "Khoa D. Doan", "Roger Wattenhofer", "Ngo Anh Vien", "Mathias Niepert", "Daniel Sonntag", "Paul Swoboda"], "abstract": "Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at this https URL", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["token merging", "3D transformers"], "summary": "This work introduces gitmerge3D, a method that reduces token count in 3D transformers by up to 95%, improving computational efficiency while maintaining performance."}, {"arxiv_id": "arXiv:2511.05432", "title": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis", "abs_url": "https://arxiv.org/abs/2511.05432", "pdf_url": "https://arxiv.org/pdf/2511.05432", "authors": ["Dogucan Yaman", "Seymanur Akti", "Fevziye Irem Eyiokur", "Alexander Waibel"], "abstract": "We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["speech", "facial synthesis"], "summary": "A text-to-talking-face framework using HierSpeech++ for tight audio-visual alignment and natural speech with synchronized facial motion."}, {"arxiv_id": "arXiv:2511.05421", "title": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration", "abs_url": "https://arxiv.org/abs/2511.05421", "pdf_url": "https://arxiv.org/pdf/2511.05421", "authors": ["Aupendu Kar", "Krishnendu Ghosh", "Prabir Kumar Biswas"], "abstract": "Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["continual learning", "image restoration"], "summary": "This paper proposes a simple convolution layer modification for continual learning in image restoration, enhancing performance on new tasks without architectural changes or significant overhead."}, {"arxiv_id": "arXiv:2511.05404", "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments", "abs_url": "https://arxiv.org/abs/2511.05404", "pdf_url": "https://arxiv.org/pdf/2511.05404", "authors": ["Laura Alejandra Encinar Gonzalez", "John Folkesson", "Rudolph Triebel", "Riccardo Giubilato"], "abstract": "Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at this http URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["MPRF", "loop closure"], "summary": "MPRF integrates vision and LiDAR data using transformers for robust loop closure in unstructured environments."}, {"arxiv_id": "arXiv:2511.05403", "title": "PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior", "abs_url": "https://arxiv.org/abs/2511.05403", "pdf_url": "https://arxiv.org/pdf/2511.05403", "authors": ["Zicong Fan", "Edoardo Remelli", "David Dimond", "Fadime Sener", "Liuhao Ge", "Bugra Tekin", "Cem Keskin", "Shreyas Hampali"], "abstract": "The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["PALM dataset", "hand avatar"], "summary": "This paper introduces PALM, a large dataset of high-quality hand scans and images, enabling realistic hand avatar personalization."}, {"arxiv_id": "arXiv:2511.05394", "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly", "abs_url": "https://arxiv.org/abs/2511.05394", "pdf_url": "https://arxiv.org/pdf/2511.05394", "authors": ["Alexander Htet Kyaw", "Haotian Ma", "Sasa Zivkovic", "Jenny Sabin"], "abstract": "We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["object recognition", "Augmented Reality"], "summary": "The paper introduces an AI-assisted AR system for assembly that uses object recognition to guide users step-by-step."}, {"arxiv_id": "arXiv:2511.05393", "title": "PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization", "abs_url": "https://arxiv.org/abs/2511.05393", "pdf_url": "https://arxiv.org/pdf/2511.05393", "authors": ["Zehui Feng", "Tian Qiu", "Tong Wu", "Junxuan Li", "Huayuan Xu", "Ting Han"], "abstract": "Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Reinforcement Learning", "Quality Assessment"], "summary": "PreResQ-R1 uses reinforcement learning to improve visual quality assessment by enhancing reasoning and score calibration across domains."}, {"arxiv_id": "arXiv:2511.05369", "title": "Dense Motion Captioning", "abs_url": "https://arxiv.org/abs/2511.05369", "pdf_url": "https://arxiv.org/pdf/2511.05369", "authors": ["Shiyao Xu", "Benedetta Liberatori", "GÃ¼l Varol", "Paolo Rota"], "abstract": "Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Dense Motion Captioning", "Complex Motion Dataset"], "summary": "The paper introduces Dense Motion Captioning and the Complex Motion Dataset, advancing 3D human motion understanding through temporally grounded captions."}, {"arxiv_id": "arXiv:2511.05356", "title": "Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects", "abs_url": "https://arxiv.org/abs/2511.05356", "pdf_url": "https://arxiv.org/pdf/2511.05356", "authors": ["Manuel Gomes", "Bogdan Raducanu", "Miguel Oliveira"], "abstract": "Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["4D panoptic segmentation", "CanonSeg4D"], "summary": "The paper introduces Artic4D, a new dataset and CanonSeg4D framework for 4D panoptic segmentation of articulated objects, improving temporal modeling and canonical alignment."}, {"arxiv_id": "arXiv:2511.05319", "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models", "abs_url": "https://arxiv.org/abs/2511.05319", "pdf_url": "https://arxiv.org/pdf/2511.05319", "authors": ["Huanqi Wu", "Huangbiao Xu", "Runfeng Xie", "Jiaxin Cai", "Kaixin Zhang", "Xiao Ke"], "abstract": "Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Cryptography and Security (cs.CR)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["{\"keywords\": [\"Sentence-to-Image Steganography\"", "\"Semantic Steganography\"]"], "summary": "Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers."}, {"arxiv_id": "arXiv:2511.05308", "title": "Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation", "abs_url": "https://arxiv.org/abs/2511.05308", "pdf_url": "https://arxiv.org/pdf/2511.05308", "authors": ["Matteo Bastico", "David Ryckelynck", "Laurent CortÃ©", "Yannick Tillier", "Etienne DecenciÃ¨re"], "abstract": "As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Chamfer Distance", "Surface Normal Concordance"], "summary": "This paper introduces Surface Normal Concordance (SNC) and a new generative model, the Diffusion Point Transformer, improving the evaluation and generation of 3D point clouds."}, {"arxiv_id": "arXiv:2511.05299", "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding", "abs_url": "https://arxiv.org/abs/2511.05299", "pdf_url": "https://arxiv.org/pdf/2511.05299", "authors": ["Zhenyu Yang", "Kairui Zhang", "Yuhang Hu", "Bing Wang", "Shengsheng Qian", "Bin Wen", "Fan Yang", "Tingting Gao", "Weiming Dong", "Changsheng Xu"], "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["LiveStar", "proactive responses"], "summary": "LiveStar enhances real-time video understanding by providing always-on proactive responses through adaptive streaming and memory-aware acceleration."}, {"arxiv_id": "arXiv:2511.05293", "title": "Cross-domain EEG-based Emotion Recognition with Contrastive Learning", "abs_url": "https://arxiv.org/abs/2511.05293", "pdf_url": "https://arxiv.org/pdf/2511.05293", "authors": ["Rui Yan", "Yibo Li", "Han Ding", "Fei Wang"], "abstract": "Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["EmotionCLIP", "SST-LegoViT"], "summary": "This paper presents EmotionCLIP and SST-LegoViT, improving EEG-based emotion recognition with multimodal contrastive learning."}, {"arxiv_id": "arXiv:2511.05292", "title": "What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs", "abs_url": "https://arxiv.org/abs/2511.05292", "pdf_url": "https://arxiv.org/pdf/2511.05292", "authors": ["Jiaxi Yin", "Pengcheng Wang", "Han Ding", "Fei Wang"], "abstract": "Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary this http URL system code is publicly available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["CuisineSense", "wearable-based detection"], "summary": "CuisineSense uses smartwatch and smart glasses to accurately detect Chinese food types during eating, addressing privacy concerns and dietary monitoring needs."}, {"arxiv_id": "arXiv:2511.05271", "title": "DeepEyesV2: Toward Agentic Multimodal Model", "abs_url": "https://arxiv.org/abs/2511.05271", "pdf_url": "https://arxiv.org/pdf/2511.05271", "authors": ["Jack Hong", "Chenxiao Zhao", "ChengLin Zhu", "Weiheng Lu", "Guohai Xu", "Xing Yu"], "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["DeepEyesV2", "tool invocation"], "summary": "This work introduces DeepEyesV2, a multimodal model trained with a two-stage pipeline to effectively invoke external tools for real-world reasoning tasks."}, {"arxiv_id": "arXiv:2511.05263", "title": "OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU", "abs_url": "https://arxiv.org/abs/2511.05263", "pdf_url": "https://arxiv.org/pdf/2511.05263", "authors": ["Qi Sun", "Dingju Zhou", "Lina Zhang"], "abstract": "The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["appearance frequency", "OregairuChar"], "summary": "This work introduces OregairuChar, a dataset for analyzing character appearance frequency in anime, to study narrative dynamics and character prominence."}, {"arxiv_id": "arXiv:2511.05253", "title": "Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection", "abs_url": "https://arxiv.org/abs/2511.05253", "pdf_url": "https://arxiv.org/pdf/2511.05253", "authors": ["Tiziano Natali", "Karin A. Olthof", "Niels F.M. Kok", "Koert F.D. Kuhlmann", "Theo J.M. Ruers", "Matteo Fusaglia"], "abstract": "Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows. Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use. Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC = 0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic segmentation but with ~4x faster execution (~ 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance. Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Image and Video Processing (eess.IV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["3D U-Net", "cropped-volume model"], "summary": "The study presents a cropped-volume 3D U-Net model that provides accurate, near real-time segmentation of colorectal liver metastases for intraoperative ultrasound navigation."}, {"arxiv_id": "arXiv:2511.05250", "title": "Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks", "abs_url": "https://arxiv.org/abs/2511.05250", "pdf_url": "https://arxiv.org/pdf/2511.05250", "authors": ["Mohamed Sanim Akremi", "Rim Slama", "Hedi Tabia"], "abstract": "Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["online recognition", "Siamese network"], "summary": "This paper introduces an online motion recognition system using SPD matrices and a Siamese network for continuous skeleton sequence analysis."}, {"arxiv_id": "arXiv:2511.05245", "title": "ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining", "abs_url": "https://arxiv.org/abs/2511.05245", "pdf_url": "https://arxiv.org/pdf/2511.05245", "authors": ["Xincheng Yao", "Yan Luo", "Zefeng Qian", "Chongyang Zhang"], "abstract": "The current mainstream and state-of-the-art anomaly detection (AD) methods are substantially established on pretrained feature networks yielded by ImageNet pretraining. However, regardless of supervised or self-supervised pretraining, the pretraining process on ImageNet does not match the goal of anomaly detection (i.e., pretraining in natural images doesn't aim to distinguish between normal and abnormal). Moreover, natural images and industrial image data in AD scenarios typically have the distribution shift. The two issues can cause ImageNet-pretrained features to be suboptimal for AD tasks. To further promote the development of the AD field, pretrained representations specially for AD tasks are eager and very valuable. To this end, we propose a novel AD representation learning framework specially designed for learning robust and discriminative pretrained representations for industrial anomaly detection. Specifically, closely surrounding the goal of anomaly detection (i.e., focus on discrepancies between normals and anomalies), we propose angle- and norm-oriented contrastive losses to maximize the angle size and norm difference between normal and abnormal features simultaneously. To avoid the distribution shift from natural images to AD images, our pretraining is performed on a large-scale AD dataset, RealIAD. To further alleviate the potential shift between pretraining data and downstream AD datasets, we learn the pretrained AD representations based on the class-generalizable representation, residual features. For evaluation, based on five embedding-based AD methods, we simply replace their original features with our pretrained representations. Extensive experiments on five AD datasets and five backbones consistently show the superiority of our pretrained features. The code is available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["anomaly detection", "pretrained representations"], "summary": "This paper proposes a novel framework for learning robust pretrained representations specifically for industrial anomaly detection, outperforming existing methods."}, {"arxiv_id": "arXiv:2511.05229", "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos", "abs_url": "https://arxiv.org/abs/2511.05229", "pdf_url": "https://arxiv.org/pdf/2511.05229", "authors": ["Mengqi Guo", "Bo Xu", "Yanyan Li", "Gim Hee Lee"], "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["4D3R", "Dynamic Neural Rendering"], "summary": "4D3R proposes a pose-free framework for novel view synthesis in dynamic scenes, combining motion-aware bundle adjustment and Gaussian splatting for improved accuracy and efficiency."}, {"arxiv_id": "arXiv:2511.05219", "title": "FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction", "abs_url": "https://arxiv.org/abs/2511.05219", "pdf_url": "https://arxiv.org/pdf/2511.05219", "authors": ["Jiang Lin", "Xinyu Chen", "Song Wu", "Zhiqiu Zhang", "Jizhi Zhang", "Ye Wang", "Qiang Tang", "Qian Wang", "Jian Yang", "Zili Yi"], "abstract": "Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["FreeControl", "Latent-Condition Decoupling"], "summary": "FreeControl offers efficient semantic structural control in diffusion models through one-step attention extraction and Latent-Condition Decoupling, enabling flexible and high-quality image generation."}, {"arxiv_id": "arXiv:2511.05210", "title": "Walk the Lines 2: Contour Tracking for Detailed Segmentation", "abs_url": "https://arxiv.org/abs/2511.05210", "pdf_url": "https://arxiv.org/pdf/2511.05210", "authors": ["AndrÃ© Peter Kelm", "Max Braeschke", "Emre GÃ¼lsoylu", "Simone Frintrop"], "abstract": "This paper presents Walk the Lines 2 (WtL2), a unique contour tracking algorithm specifically adapted for detailed segmentation of infrared (IR) ships and various objects in RGB.1 This extends the original Walk the Lines (WtL) [12], which focused solely on detailed ship segmentation in color. These innovative WtLs can replace the standard non-maximum suppression (NMS) by using contour tracking to refine the object contour until a 1-pixel-wide closed shape can be binarized, forming a segmentable area in foreground-background scenarios. WtL2 broadens the application range of WtL beyond its original scope, adapting to IR and expanding to diverse objects within the RGB context. To achieve IR segmentation, we adapt its input, the object contour detector, to IR ships. In addition, the algorithm is enhanced to process a wide range of RGB objects, outperforming the latest generation of contour-based methods when achieving a closed object contour, offering high peak Intersection over Union (IoU) with impressive details. This positions WtL2 as a compelling method for specialized applications that require detailed segmentation or high-quality samples, potentially accelerating progress in several niche areas of image segmentation.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Walk the Lines 2", "contour tracking"], "summary": "WtL2 enhances contour tracking for detailed segmentation of IR ships and diverse RGB objects, outperforming current methods."}, {"arxiv_id": "arXiv:2511.05170", "title": "MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification", "abs_url": "https://arxiv.org/abs/2511.05170", "pdf_url": "https://arxiv.org/pdf/2511.05170", "authors": ["Zijiang Yang", "Hanqing Chao", "Bokai Zhao", "Yelin Yang", "Yunshuo Zhang", "Dongmei Fu", "Junping Zhang", "Le Lu", "Ke Yan", "Dakai Jin", "Minfeng Xu", "Yun Bian", "Hui Jiang"], "abstract": "Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Nucleus detection", "self-distillation"], "summary": "MUSE proposes NuLo for flexible local self-distillation and achieves superior performance in nucleus detection and classification without strict spatial alignment."}, {"arxiv_id": "arXiv:2511.05168", "title": "Another BRIXEL in the Wall: Towards Cheaper Dense Features", "abs_url": "https://arxiv.org/abs/2511.05168", "pdf_url": "https://arxiv.org/pdf/2511.05168", "authors": ["Alexander Lappe", "Martin A. Giese"], "abstract": "Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["BRIXEL", "knowledge distillation"], "summary": "BRIXEL uses knowledge distillation to produce high-resolution feature maps similar to DINOv3 at lower computational cost."}, {"arxiv_id": "arXiv:2511.05152", "title": "Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges", "abs_url": "https://arxiv.org/abs/2511.05152", "pdf_url": "https://arxiv.org/pdf/2511.05152", "authors": ["Adrian Azzarelli", "Nantheera Anantrasirichai", "David R Bull"], "abstract": "Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t=0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: this https URL", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Graphics (cs.GR)", "Multimedia (cs.MM)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Deformable Gaussian Splatting", "Foreground-Background Splitting"], "summary": "This paper introduces a method for dynamic 3-D reconstruction using a foreground-background splitting technique to handle sparse camera setups in filmmaking."}, {"arxiv_id": "arXiv:2511.05150", "title": "From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection", "abs_url": "https://arxiv.org/abs/2511.05150", "pdf_url": "https://arxiv.org/pdf/2511.05150", "authors": ["Jingsong Liu", "Han Li", "Nassir Navab", "Peter J. SchÃ¼ffler"], "abstract": "AI-based biomarkers can infer molecular features directly from hematoxylin & eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["JWTH", "cell-centric"], "summary": "The paper introduces JWTH, a PFM model that improves accuracy and interpretability in AI-based biomarker detection in digital pathology."}, {"arxiv_id": "arXiv:2511.05108", "title": "SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements", "abs_url": "https://arxiv.org/abs/2511.05108", "pdf_url": "https://arxiv.org/pdf/2511.05108", "authors": ["JÃ¶rg Gamerdinger", "Benedict Wetzel", "Patrick Schulz", "Sven Teufel", "Oliver Bringmann"], "abstract": "Lane detection for autonomous driving in snow-covered environments remains a major challenge due to the frequent absence or occlusion of lane markings. In this paper, we present a novel, robust and realtime capable approach that bypasses the reliance on traditional lane markings by detecting roadside features,specifically vertical roadside posts called delineators, as indirect lane indicators. Our method first perceives these posts, then fits a smooth lane trajectory using a parameterized Bezier curve model, leveraging spatial consistency and road geometry. To support training and evaluation in these challenging scenarios, we introduce SnowyLane, a new synthetic dataset containing 80,000 annotated frames capture winter driving conditions, with varying snow coverage, and lighting conditions. Compared to state-of-the-art lane detection systems, our approach demonstrates significantly improved robustness in adverse weather, particularly in cases with heavy snow occlusion. This work establishes a strong foundation for reliable lane detection in winter scenarios and contributes a valuable resource for future research in all-weather autonomous driving. The dataset is available at this https URL", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["delineators", "Bezier curve"], "summary": "A novel approach uses roadside delineators and Bezier curves to detect lanes in snow-covered environments, enhancing robustness for autonomous driving."}, {"arxiv_id": "arXiv:2511.05106", "title": "Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study", "abs_url": "https://arxiv.org/abs/2511.05106", "pdf_url": "https://arxiv.org/pdf/2511.05106", "authors": ["Yasemin Turkan", "F. Boray Tek", "M. Serdar NazlÄ±", "Ã–ykÃ¼ Eren"], "abstract": "Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer's disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["OCT B-scans", "AD prediction"], "summary": "This study uses deep learning on raw OCT B-scans to predict Alzheimer's disease, achieving an AUC of 0.62, providing a baseline for future research."}, {"arxiv_id": "arXiv:2511.05095", "title": "Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start", "abs_url": "https://arxiv.org/abs/2511.05095", "pdf_url": "https://arxiv.org/pdf/2511.05095", "authors": ["Fuyang Liu", "Jiaqi Xu", "Xiaowei Hu"], "abstract": "Adverse weather severely impairs real-world visual perception, while existing vision models trained on synthetic data with fixed parameters struggle to generalize to complex degradations. To address this, we first construct HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse weather phenomena, and then design a dual-level reinforcement learning framework initialized with HFLS-Weather for cold-start training. Within this framework, at the local level, weather-specific restoration models are refined through perturbation-driven image quality optimization, enabling reward-based learning without paired supervision; at the global level, a meta-controller dynamically orchestrates model selection and execution order according to scene degradation. This framework enables continuous adaptation to real-world conditions and achieves state-of-the-art performance across a wide range of adverse weather scenarios. Code is available at this https URL", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["weather", "reinforcement learning"], "summary": "This paper presents a dual-level reinforcement learning framework using a physics-driven dataset to improve visual perception under adverse weather conditions."}, {"arxiv_id": "arXiv:2511.05092", "title": "A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification", "abs_url": "https://arxiv.org/abs/2511.05092", "pdf_url": "https://arxiv.org/pdf/2511.05092", "authors": ["Ruolin Li", "Min Liu", "Yuan Bian", "Zhaoyang Li", "Yuzhen Li", "Xueping Wang", "Yaonan Wang"], "abstract": "With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["GenePerson", "Prompt-driven Disentanglement Mechanism"], "summary": "The paper introduces GenePerson, a large-scale virtual dataset, and PDM, a mechanism for learning domain-invariant features, improving person re-identification model generalization."}, {"arxiv_id": "arXiv:2511.05073", "title": "Deep learning models are vulnerable, but adversarial examples are even more vulnerable", "abs_url": "https://arxiv.org/abs/2511.05073", "pdf_url": "https://arxiv.org/pdf/2511.05073", "authors": ["Jun Li", "Yanwei Xu", "Keran Li", "Xiaoli Zhang"], "abstract": "Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["occlusion sensitivity", "adversarial example detection"], "summary": "This paper identifies that adversarial examples are more sensitive to occlusion than clean images and proposes a new method, SWM-AED, for detecting adversarial examples."}, {"arxiv_id": "arXiv:2511.05059", "title": "SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery", "abs_url": "https://arxiv.org/abs/2511.05059", "pdf_url": "https://arxiv.org/pdf/2511.05059", "authors": ["Mingyu Sheng", "Jianan Fan", "Dongnan Liu", "Guoyan Zheng", "Ron Kikinis", "Weidong Cai"], "abstract": "During laparoscopic surgery, smoke generated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision-making and computer-assisted visual analysis. Consequently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data-driven deep learning models, combining the superior generalizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical requirements. It introduces only two hyperparameters and no additional trainable weights, preserving the original network architecture with minimal computational and modification overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking methods, involving multiple network architectures and covering diverse procedures, including cholecystectomy, partial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly reduces the restoration errors of existing models and relatively enhances their generalizability, without adding any trainable layers or weights. This highlights the convenience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Surgical Atmospheric Model", "SurgiATM"], "summary": "The paper introduces SurgiATM, a lightweight module that enhances surgical smoke removal accuracy and stability in various desmoking architectures."}, {"arxiv_id": "arXiv:2511.05057", "title": "Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach", "abs_url": "https://arxiv.org/abs/2511.05057", "pdf_url": "https://arxiv.org/pdf/2511.05057", "authors": ["Yuanxiang Huangfu", "Chaochao Wang", "Weilei Wang"], "abstract": "The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Role-SynthCLIP", "multi-perspective role-playing"], "summary": "A novel framework using multi-perspective role-playing to enhance semantic diversity in CLIP training data, significantly improving caption expressiveness and accuracy."}, {"arxiv_id": "arXiv:2511.05055", "title": "No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation", "abs_url": "https://arxiv.org/abs/2511.05055", "pdf_url": "https://arxiv.org/pdf/2511.05055", "authors": ["Mingyu Sung", "Hyeonmin Choe", "Il-Min Kim", "Sangseok Yun", "Jae Mo Kang"], "abstract": "Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["PITTA", "Test-time adaptation"], "summary": "The paper introduces PITTA, a novel framework for test-time adaptation in monocular depth estimation that enhances performance in diverse environments."}, {"arxiv_id": "arXiv:2511.05044", "title": "Medical Referring Image Segmentation via Next-Token Mask Prediction", "abs_url": "https://arxiv.org/abs/2511.05044", "pdf_url": "https://arxiv.org/pdf/2511.05044", "authors": ["Xinyu Chen", "Yiran Wang", "Gaoyang Pang", "Jiafu Hao", "Chentao Yue", "Luping Zhou", "Yonghui Li"], "abstract": "Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["NTP-MRISeg", "Next-token prediction"], "summary": "NTP-MRISeg reformulates medical referring image segmentation as a next-token prediction task, improving performance and simplifying model design."}, {"arxiv_id": "arXiv:2511.05038", "title": "Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance", "abs_url": "https://arxiv.org/abs/2511.05038", "pdf_url": "https://arxiv.org/pdf/2511.05038", "authors": ["Zhengxuan Li", "Qinhui Yang", "Yiyu Zhuang", "Chuan Guo", "Xinxin Zuo", "Xiaoxiao Long", "Yao Yao", "Xun Cao", "Qiu Shen", "Hao Zhu"], "abstract": "We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Pressure2Motion", "motion capture"], "summary": "Pressure2Motion synthesizes human motion from ground pressure sequences and text prompts, enabling privacy-preserving, low-cost motion capture."}, {"arxiv_id": "arXiv:2511.05034", "title": "Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation", "abs_url": "https://arxiv.org/abs/2511.05034", "pdf_url": "https://arxiv.org/pdf/2511.05034", "authors": ["Jing Jin", "Xu Liu", "Te Gao", "Zhihong Shi", "Yixiong Liang", "Ruiqing Zheng", "Hulin Kuang", "Min Zeng", "Shichao Kan"], "abstract": "Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation this http URL an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Dynamic Residual Encoding", "Contrastive Learning"], "summary": "The paper introduces DRE-SLCL, an end-to-end method for WSI representation using dynamic residual encoding and contrastive learning to handle GPU limitations."}, {"arxiv_id": "arXiv:2511.05017", "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings", "abs_url": "https://arxiv.org/abs/2511.05017", "pdf_url": "https://arxiv.org/pdf/2511.05017", "authors": ["Aakriti Agrawal", "Gouthaman KV", "Rohith Aralikatti", "Gauri Jagatap", "Jiaxin Yuan", "Vijay Kamarshi", "Andrea Fanelli", "Furong Huang"], "abstract": "In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["visual grounding", "modality imbalance"], "summary": "The paper addresses the language bias in LVLMs by refining textual embeddings with visual features, improving visual grounding and reducing hallucinations."}, {"arxiv_id": "arXiv:2511.04977", "title": "GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder", "abs_url": "https://arxiv.org/abs/2511.04977", "pdf_url": "https://arxiv.org/pdf/2511.04977", "authors": ["Heng Er Metilda Chee", "Jiayin Wang", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "abstract": "Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Multimedia (cs.MM)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Sticker Semantic Similarity", "Triple-S"], "summary": "This paper introduces the Sticker Semantic Similarity task and the Triple-S benchmark, along with a new model called General Sticker Encoder (GSE) that improves sticker understanding and retrieval."}, {"arxiv_id": "arXiv:2511.04972", "title": "Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features", "abs_url": "https://arxiv.org/abs/2511.04972", "pdf_url": "https://arxiv.org/pdf/2511.04972", "authors": ["Dylan Peek", "Matthew P. Skerritt", "Siddharth Pritam", "Stephan Chalup"], "abstract": "Topological Data Analysis (TDA) involves techniques of analyzing the underlying structure and connectivity of data. However, traditional methods like persistent homology can be computationally demanding, motivating the development of neural network-based estimators capable of reducing computational overhead and inference time. A key barrier to advancing these methods is the lack of labeled 3D data with class distributions and diversity tailored specifically for supervised learning in TDA tasks. To address this, we introduce a novel approach for systematically generating labeled 3D datasets using the Repulsive Surface algorithm, allowing control over topological invariants, such as hole count. The resulting dataset offers varied geometry with topological labeling, making it suitable for training and benchmarking neural network estimators. This paper uses a synthetic 3D dataset to train a genus estimator network, created using a 3D convolutional transformer architecture. An observed decrease in accuracy as deformations increase highlights the role of not just topological complexity, but also geometric complexity, when training generalized estimators. This dataset fills a gap in labeled 3D datasets and generation for training and evaluating models and techniques for TDA.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["3D datasets", "topological invariants"], "summary": "This paper introduces a method for generating labeled 3D datasets to train and benchmark neural network estimators in Topological Data Analysis."}, {"arxiv_id": "arXiv:2511.04970", "title": "Learning Fourier shapes to probe the geometric world of deep neural networks", "abs_url": "https://arxiv.org/abs/2511.04970", "pdf_url": "https://arxiv.org/pdf/2511.04970", "authors": ["Jian Wang", "Yixing Yong", "Haixia Bi", "Lijun He", "Fan Li"], "abstract": "While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["shapes", "semantic carriers"], "summary": "This research demonstrates that optimized shapes can act as potent semantic carriers, precisely isolating a model's salient regions and constituting a new adversarial paradigm."}, {"arxiv_id": "arXiv:2511.04963", "title": "Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement", "abs_url": "https://arxiv.org/abs/2511.04963", "pdf_url": "https://arxiv.org/pdf/2511.04963", "authors": ["Xiongri Shen", "Jiaqi Wang", "Yi Zhong", "Zhenxi Song", "Leilei Zhao", "Yichen Wei", "Lingyan Liang", "Shuqiang Wang", "Baiying Lei", "Demao Deng", "Zhiguo Zhang"], "abstract": "Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\\% for fMRI synthesis (+1.54 dB/+4.12\\% over baselines) and 30.00 dB/77.55\\% for dMRI synthesis (+1.02 dB/+2.2\\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\\%/66.02\\%/64.15\\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \\href{ this https URL }{PDS GitHub Repository}", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["fMRI-dMRI synthesis", "PDS"], "summary": "The paper introduces PDS, a method for fMRI-dMRI synthesis that improves accuracy and maintains structural fidelity, outperforming existing approaches."}, {"arxiv_id": "arXiv:2511.04951", "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting", "abs_url": "https://arxiv.org/abs/2511.04951", "pdf_url": "https://arxiv.org/pdf/2511.04951", "authors": ["Hexu Zhao", "Xiwen Min", "Xiaoteng Liu", "Moonjun Gong", "Yiming Li", "Ang Li", "Saining Xie", "Jinyang Li", "Aurojit Panda"], "abstract": "3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["3D Gaussian Splatting", "offloading"], "summary": "CLM enables 3D Gaussian Splatting to render large scenes using offloading techniques on a single GPU."}, {"arxiv_id": "arXiv:2511.04949", "title": "DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning", "abs_url": "https://arxiv.org/abs/2511.04949", "pdf_url": "https://arxiv.org/pdf/2511.04949", "authors": ["Tharindu Fernando", "Clinton Fookes", "Sridha Sridharan"], "abstract": "Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["watermarking", "MAARL"], "summary": "This paper presents a novel deep learning framework using MAARL and latent space watermarking to enhance robustness and adaptability in deepfake detection."}, {"arxiv_id": "arXiv:2511.04948", "title": "A benchmark multimodal oro-dental dataset for large vision-language models", "abs_url": "https://arxiv.org/abs/2511.04948", "pdf_url": "https://arxiv.org/pdf/2511.04948", "authors": ["Haoxin Lv", "Ijazul Haq", "Jin Du", "Jiaxin Ma", "Binnian Zhu", "Xiaobing Dang", "Chaoan Liang", "Ruxu Du", "Yingjie Zhang", "Muhammad Saqib"], "abstract": "The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["multimodal dataset", "AI dentistry"], "summary": "This paper introduces a large multimodal dataset for AI in dentistry, enhancing the performance of vision-language models in diagnosing oral health issues and generating reports."}, {"arxiv_id": "arXiv:2511.04920", "title": "Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation", "abs_url": "https://arxiv.org/abs/2511.04920", "pdf_url": "https://arxiv.org/pdf/2511.04920", "authors": ["Hu Gao", "Xiaoning Lei", "Ying Zhang", "Xichen Xu", "Guannan Jiang", "Lizhuang Ma"], "abstract": "Image restoration (IR) aims to recover clean images from degraded observations. Despite remarkable progress, most existing methods focus on a single degradation type, whereas real-world images often suffer from multiple coexisting degradations, such as rain, noise, and haze coexisting in a single image, which limits their practical effectiveness. In this paper, we propose an adaptive multi-degradation image restoration network that reconstructs images by leveraging decoupled representations of degradation ingredients to guide path selection. Specifically, we design a degradation ingredient decoupling block (DIDBlock) in the encoder to separate degradation ingredients statistically by integrating spatial and frequency domain information, enhancing the recognition of multiple degradation types and making their feature representations independent. In addition, we present fusion block (FBlock) to integrate degradation information across all levels using learnable matrices. In the decoder, we further introduce a task adaptation block (TABlock) that dynamically activates or fuses functional branches based on the multi-degradation representation, flexibly selecting optimal restoration paths under diverse degradation conditions. The resulting tightly integrated architecture, termed IMDNet, is extensively validated through experiments, showing superior performance on multi-degradation restoration while maintaining strong competitiveness on single-degradation tasks.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["multi-degradation", "IMDNet"], "summary": "This paper introduces IMDNet, an adaptive network for image restoration that effectively handles multiple coexisting degradations."}, {"arxiv_id": "arXiv:2511.04886", "title": "Beta Distribution Learning for Reliable Roadway Crash Risk Assessment", "abs_url": "https://arxiv.org/abs/2511.04886", "pdf_url": "https://arxiv.org/pdf/2511.04886", "authors": ["Ahmad Elallaf", "Nathan Jacobs", "Xinyue Ye", "Mei Chen", "Gongbo Liang"], "abstract": "Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions--a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["satellite imagery", "Beta probability distribution"], "summary": "A geospatial deep learning framework using satellite imagery to estimate fatal crash risks with uncertainty, improving safety assessments and applications."}, {"arxiv_id": "arXiv:2511.04872", "title": "Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects", "abs_url": "https://arxiv.org/abs/2511.04872", "pdf_url": "https://arxiv.org/pdf/2511.04872", "authors": ["James Ndubuisi", "Fernando Auat", "Marta Vallejo"], "abstract": "This study evaluates the efficacy of vision transformer models, specifically Swin transformers, in enhancing the diagnostic accuracy of ear diseases compared to traditional convolutional neural networks. With a reported 27% misdiagnosis rate among specialist otolaryngologists, improving diagnostic accuracy is crucial. The research utilised a real-world dataset from the Department of Otolaryngology at the Clinical Hospital of the Universidad de Chile, comprising otoscopic videos of ear examinations depicting various middle and external ear conditions. Frames were selected based on the Laplacian and Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively, marginally outperforming the ResNet model (99.5%). These results surpassed metrics reported in related studies. However, the evaluation uncovered a critical data leakage issue in the preprocessing step, affecting both this study and related research using the same raw dataset. After mitigating the data leakage, model performance decreased significantly. Corrected accuracies were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This finding highlights the importance of rigorous data handling in machine learning studies, especially in medical applications. The findings indicate that while vision transformers show promise, it is essential to find an optimal balance between the benefits of advanced model architectures and those derived from effective data preprocessing. This balance is key to developing a reliable machine learning model for diagnosing ear diseases.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Swin transformers", "diagnostic accuracy"], "summary": "This study compares Swin transformers with traditional CNNs for diagnosing ear diseases, highlighting the importance of data preprocessing to achieve reliable results."}, {"arxiv_id": "arXiv:2511.04871", "title": "Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications", "abs_url": "https://arxiv.org/abs/2511.04871", "pdf_url": "https://arxiv.org/pdf/2511.04871", "authors": ["Gabriel Girard", "Manon Edde", "FÃ©lix Dumais", "Yoan David", "Matthieu Dumont", "Guillaume Theaud", "Jean-Christophe Houde", "Arnaud BorÃ©", "Maxime Descoteaux", "Pierre-Marc Jodoin"], "abstract": "Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Applications (stat.AP)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Clinical-ComBAT", "ComBAT"], "summary": "This paper introduces Clinical-ComBAT, an enhanced method for harmonizing diffusion-weighted MRI data across multiple sites, improving its clinical applicability."}, {"arxiv_id": "arXiv:2511.04864", "title": "Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction", "abs_url": "https://arxiv.org/abs/2511.04864", "pdf_url": "https://arxiv.org/pdf/2511.04864", "authors": ["Kyle Fogarty", "Chenyue Cai", "Jing Yang", "Zhilin Guo", "Cengiz Ã–ztireli"], "abstract": "Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["implicit self-prior", "point cloud"], "summary": "A method using an implicit self-prior from point clouds to generate high-fidelity surfaces with detailed preservation."}, {"arxiv_id": "arXiv:2511.04848", "title": "Geometry Denoising with Preferred Normal Vectors", "abs_url": "https://arxiv.org/abs/2511.04848", "pdf_url": "https://arxiv.org/pdf/2511.04848", "authors": ["Manuel WeiÃŸ", "Lukas BaumgÃ¤rtner", "Roland Herzog", "Stephan Schmidt"], "abstract": "We introduce a new paradigm for geometry denoising using prior knowledge about the surface normal vector. This prior knowledge comes in the form of a set of preferred normal vectors, which we refer to as label vectors. A segmentation problem is naturally embedded in the denoising process. The segmentation is based on the similarity of the normal vector to the elements of the set of label vectors. Regularization is achieved by a total variation term. We formulate a split Bregman (ADMM) approach to solve the resulting optimization problem. The vertex update step is based on second-order shape calculus.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Optimization and Control (math.OC)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["geometry denoising", "label vectors"], "summary": "A new geometry denoising method uses label vectors and a split Bregman approach for surface normal vector refinement."}, {"arxiv_id": "arXiv:2511.04811", "title": "An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention", "abs_url": "https://arxiv.org/abs/2511.04811", "pdf_url": "https://arxiv.org/pdf/2511.04811", "authors": ["Shuo Zhao", "Yu Zhou", "Jianxu Chen"], "abstract": "Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["active learning", "pseudo-labeling"], "summary": "This work proposes a data-centric AI workflow using active learning and pseudo-labeling to improve biomedical image segmentation with minimal manual annotation."}, {"arxiv_id": "arXiv:2511.04803", "title": "Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose", "abs_url": "https://arxiv.org/abs/2511.04803", "pdf_url": "https://arxiv.org/pdf/2511.04803", "authors": ["Shuo Zhao", "Jianxu Chen"], "abstract": "Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at this https URL .", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["data redundancy", "catastrophic forgetting"], "summary": "This study explores data redundancy and catastrophic forgetting in biomedical image segmentation using Cellpose, proposing strategies to mitigate these challenges."}, {"arxiv_id": "arXiv:2511.04797", "title": "3D Gaussian Point Encoders", "abs_url": "https://arxiv.org/abs/2511.04797", "pdf_url": "https://arxiv.org/pdf/2511.04797", "authors": ["Jim James", "Ben Wilson", "Simon Lucey", "James Hays"], "abstract": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["3D Gaussian Point Encoder", "PointNet"], "summary": "This paper introduces 3D Gaussian Point Encoders, a faster and more parameter-efficient alternative to PointNets for 3D recognition tasks."}, {"arxiv_id": "arXiv:2511.04779", "title": "EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear", "abs_url": "https://arxiv.org/abs/2511.04779", "pdf_url": "https://arxiv.org/pdf/2511.04779", "authors": ["Andrea Aspesi (1 and 2)", "Andrea Simpsi (1)", "Aaron Tognoli (1)", "Simone Mentasti (1)", "Luca Merigo (2)", "Matteo Matteucci (1) ((1) Department of Electronics", "Information and Bioengineering (DEIB) Politecnico di Milano", "(2) EssilorLuxottica)"], "abstract": "Event-based cameras are becoming a popular solution for efficient, low-power eye tracking. Due to the sparse and asynchronous nature of event data, they require less processing power and offer latencies in the microsecond range. However, many existing solutions are limited to validation on powerful GPUs, with no deployment on real embedded devices. In this paper, we present EETnet, a convolutional neural network designed for eye tracking using purely event-based data, capable of running on microcontrollers with limited resources. Additionally, we outline a methodology to train, evaluate, and quantize the network using a public dataset. Finally, we propose two versions of the architecture: a classification model that detects the pupil on a grid superimposed on the original image, and a regression model that operates at the pixel level.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["event-based cameras", "EETnet"], "summary": "This paper introduces EETnet, a convolutional neural network for eye tracking using event-based data, designed to run on resource-limited microcontrollers."}, {"arxiv_id": "arXiv:2511.04773", "title": "Global 3D Reconstruction of Clouds & Tropical Cyclones", "abs_url": "https://arxiv.org/abs/2511.04773", "pdf_url": "https://arxiv.org/pdf/2511.04773", "authors": ["Shirin Ermis", "Cesar Aybar", "Lilli Freischem", "Stella Girtsou", "Kyriaki-Margarita Bintsi", "Emiliano Diaz Salas-Porras", "Michael Eisinger", "William Jones", "Anna Jungbluth", "Benoit Tremblay"], "abstract": "Accurate forecasting of tropical cyclones (TCs) remains challenging due to limited satellite observations probing TC structure and difficulties in resolving cloud properties involved in TC intensification. Recent research has demonstrated the capabilities of machine learning methods for 3D cloud reconstruction from satellite observations. However, existing approaches have been restricted to regions where TCs are uncommon, and are poorly validated for intense storms. We introduce a new framework, based on a pre-training--fine-tuning pipeline, that learns from multiple satellites with global coverage to translate 2D satellite imagery into 3D cloud maps of relevant cloud properties. We apply our model to a custom-built TC dataset to evaluate performance in the most challenging and relevant conditions. We show that we can - for the first time - create global instantaneous 3D cloud maps and accurately reconstruct the 3D structure of intense storms. Our model not only extends available satellite observations but also provides estimates when observations are missing entirely. This is crucial for advancing our understanding of TC intensification and improving forecasts.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Atmospheric and Oceanic Physics (physics.ao-ph)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["3D cloud maps", "tropical cyclones"], "summary": "A new framework creates global 3D cloud maps from 2D satellite imagery, enhancing the understanding and forecasting of intense tropical cyclones."}, {"arxiv_id": "arXiv:2511.04766", "title": "DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation", "abs_url": "https://arxiv.org/abs/2511.04766", "pdf_url": "https://arxiv.org/pdf/2511.04766", "authors": ["Dhenenjay Yadav", "Rohan Sawai"], "abstract": "Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN's optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Dynamic Adaptive Regularization Networks", "Adaptive Dropout Modulation"], "summary": "The paper introduces DARN, a novel decoder architecture that enhances geospatial analysis by dynamically adjusting regularization strategies based on task complexity."}, {"arxiv_id": "arXiv:2511.04753", "title": "CPO: Condition Preference Optimization for Controllable Image Generation", "abs_url": "https://arxiv.org/abs/2511.04753", "pdf_url": "https://arxiv.org/pdf/2511.04753", "authors": ["Zonglin Lyu", "Ming Li", "Xinxin Liu", "Chen Chen"], "abstract": "To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\\mathbf{c}^{w}$ and $\\mathbf{c}^{l}$, and train the model to prefer $\\mathbf{c}^{w}$. This method, which we term \\textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\\%$ error rate reduction in segmentation, $70$--$80\\%$ in human pose, and consistent $2$--$5\\%$ reductions in edge and depth maps.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["ControlNet", "Condition Preference Optimization"], "summary": "The paper introduces Condition Preference Optimization (CPO) to improve controllability in text-to-image generation, outperforming existing methods like ControlNet++."}, {"arxiv_id": "arXiv:2511.04729", "title": "Knowledge-based anomaly detection for identifying network-induced shape artifacts", "abs_url": "https://arxiv.org/abs/2511.04729", "pdf_url": "https://arxiv.org/pdf/2511.04729", "authors": ["Rucha Deshpande", "Tahsin Rahman", "Miguel Lago", "Adarsh Subbaswamy", "Jana G. Delfino", "Ghada Zamzmi", "Elim Thompson", "Aldo Badano", "Seyed Kahaki"], "abstract": "Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["anomaly detection", "synthetic data"], "summary": "This work presents a novel method for detecting shape artifacts in synthetic medical images, enhancing the quality and clinical utility of such data."}, {"arxiv_id": "arXiv:2511.04727", "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs", "abs_url": "https://arxiv.org/abs/2511.04727", "pdf_url": "https://arxiv.org/pdf/2511.04727", "authors": ["Ali Faraz", "Akash", "Shaharukh Khan", "Raja Kolla", "Akshat Patidar", "Suranjan Goswami", "Abhinav Ravi", "Chandra Khatri", "Shubham Agarwal"], "abstract": "Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.", "primary_subject": "Computer Vision and Pattern Recognition (cs.CV)", "subjects": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["IndicVisionBench", "multilingual benchmarks"], "summary": "Introduces IndicVisionBench, a large-scale benchmark for evaluating vision-language models in culturally diverse and multilingual settings."}, {"arxiv_id": "arXiv:2511.05480", "title": "On Flow Matching KL Divergence", "abs_url": "https://arxiv.org/abs/2511.05480", "pdf_url": "https://arxiv.org/pdf/2511.05480", "authors": ["Maojiang Su", "Jerry Yao-Chieh Hu", "Sophia Pi", "Han Liu"], "abstract": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\\epsilon^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \\epsilon + A_2 \\epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (stat.ML)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Kullback-Leibler divergence", "flow-matching"], "summary": "The paper derives a bound on the Kullback-Leibler divergence for flow-matching distributions, showing it achieves nearly minimax-optimal efficiency."}, {"arxiv_id": "arXiv:2511.05462", "title": "SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning", "abs_url": "https://arxiv.org/abs/2511.05462", "pdf_url": "https://arxiv.org/pdf/2511.05462", "authors": ["Xiaodong Wang", "Jing Huang", "Kevin J Liang"], "abstract": "Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["clustering", "mixture models"], "summary": "This paper introduces SiamMM, a novel clustering method inspired by mixture models, which improves performance on self-supervised learning benchmarks."}, {"arxiv_id": "arXiv:2511.05397", "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation", "abs_url": "https://arxiv.org/abs/2511.05397", "pdf_url": "https://arxiv.org/pdf/2511.05397", "authors": ["Samarth Chopra", "Alex McMoil", "Ben Carnovale", "Evan Sokolson", "Rajkumar Kubendran", "Samuel Dickerson"], "abstract": "While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: this https URL", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["EverydayVLA", "adaptive-horizon ensemble"], "summary": "EverydayVLA uses a cost-effective manipulator and an adaptive model to achieve state-of-the-art success rates in VLA tasks."}, {"arxiv_id": "arXiv:2511.05360", "title": "Neural Image Abstraction Using Long Smoothing B-Splines", "abs_url": "https://arxiv.org/abs/2511.05360", "pdf_url": "https://arxiv.org/pdf/2511.05360", "authors": ["Daniel Berio", "Michael Stroh", "Sylvain Calinon", "Frederic Fol Leymarie", "Oliver Deussen", "Ariel Shamir"], "abstract": "We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.", "primary_subject": "Graphics (cs.GR)", "subjects": ["Graphics (cs.GR)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["B-splines", "vector graphics"], "summary": "This paper integrates B-splines into DiffVG to generate smooth paths and stylized vector graphics with control over fidelity and simplicity."}, {"arxiv_id": "arXiv:2511.05183", "title": "PySlyde: A Lightweight, Open-Source Toolkit for Pathology Preprocessing", "abs_url": "https://arxiv.org/abs/2511.05183", "pdf_url": "https://arxiv.org/pdf/2511.05183", "authors": ["Gregory Verghese", "Anthony Baptista", "Chima Eke", "Holly Rafique", "Mengyuan Li", "Fathima Mohamed", "Ananya Bhalla", "Lucy Ryan", "Michael Pitcher", "Enrico Parisini", "Concetta Piazzese", "Liz Ing-Simmons", "Anita Grigoriadis"], "abstract": "The integration of artificial intelligence (AI) into pathology is advancing precision medicine by improving diagnosis, treatment planning, and patient outcomes. Digitised whole-slide images (WSIs) capture rich spatial and morphological information vital for understanding disease biology, yet their gigapixel scale and variability pose major challenges for standardisation and analysis. Robust preprocessing, covering tissue detection, tessellation, stain normalisation, and annotation parsing is critical but often limited by fragmented and inconsistent workflows. We present PySlyde, a lightweight, open-source Python toolkit built on OpenSlide to simplify and standardise WSI preprocessing. PySlyde provides an intuitive API for slide loading, annotation management, tissue detection, tiling, and feature extraction, compatible with modern pathology foundation models. By unifying these processes, it streamlines WSI preprocessing, enhances reproducibility, and accelerates the generation of AI-ready datasets, enabling researchers to focus on model development and downstream analysis.", "primary_subject": "Quantitative Methods (q-bio.QM)", "subjects": ["Quantitative Methods (q-bio.QM)", "Computer Vision and Pattern Recognition (cs.CV)", "Image and Video Processing (eess.IV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["PySlyde", "WSI preprocessing"], "summary": "PySlyde is an open-source toolkit that simplifies and standardizes whole-slide image preprocessing for AI in pathology."}, {"arxiv_id": "arXiv:2511.05102", "title": "Quantifying the Risk of Transferred Black Box Attacks", "abs_url": "https://arxiv.org/abs/2511.05102", "pdf_url": "https://arxiv.org/pdf/2511.05102", "authors": ["Disesdi Susanna Cox", "Niklas Bunzel"], "abstract": "Neural networks have become pervasive across various applications, including security-related products. However, their widespread adoption has heightened concerns regarding vulnerability to adversarial attacks. With emerging regulations and standards emphasizing security, organizations must reliably quantify risks associated with these attacks, particularly regarding transferred adversarial attacks, which remain challenging to evaluate accurately. This paper investigates the complexities involved in resilience testing against transferred adversarial attacks. Our analysis specifically addresses black-box evasion attacks, highlighting transfer-based attacks due to their practical significance and typically high transferability between neural network models. We underline the computational infeasibility of exhaustively exploring high-dimensional input spaces to achieve complete test coverage. As a result, comprehensive adversarial risk mapping is deemed impractical. To mitigate this limitation, we propose a targeted resilience testing framework that employs surrogate models strategically selected based on Centered Kernel Alignment (CKA) similarity. By leveraging surrogate models exhibiting both high and low CKA similarities relative to the target model, the proposed approach seeks to optimize coverage of adversarial subspaces. Risk estimation is conducted using regression-based estimators, providing organizations with realistic and actionable risk quantification.", "primary_subject": "Cryptography and Security (cs.CR)", "subjects": ["Cryptography and Security (cs.CR)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["adversarial attacks", "surrogate models"], "summary": "This paper proposes a targeted resilience testing framework using surrogate models to evaluate risks of transferred adversarial attacks on neural networks."}, {"arxiv_id": "arXiv:2511.05020", "title": "DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval", "abs_url": "https://arxiv.org/abs/2511.05020", "pdf_url": "https://arxiv.org/pdf/2511.05020", "authors": ["Yawei Cai", "Jiapeng Mi", "Nan Ji", "Haotian Rong", "Yawei Zhang", "Zhangti Li", "Wenbin Guo", "Rensong Xie"], "abstract": "Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can't see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.", "primary_subject": "Graphics (cs.GR)", "subjects": ["Graphics (cs.GR)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Dynamic Adaptive Fusion", "Complementary Models"], "summary": "The paper introduces DAFM, a method that enhances composed image retrieval by dynamically balancing the contributions of multiple models, improving accuracy on benchmark datasets."}, {"arxiv_id": "arXiv:2511.05009", "title": "UHDRes: Ultra-High-Definition Image Restoration via Dual-Domain Decoupled Spectral Modulation", "abs_url": "https://arxiv.org/abs/2511.05009", "pdf_url": "https://arxiv.org/pdf/2511.05009", "authors": ["S. Zhao (1)", "W. Lu (1 and 2)", "B. Wang (1)", "T. Wang (3)", "K. Zhang (4)", "H. Zhao (1) ((1) College of Computer Science and Artificial Intelligence", "Wenzhou University", "Wenzhou", "China", "(2) Nasdaq", "St. John's", "Canada", "(3) vivo Mobile Communication Co.", "Ltd", "Shanghai", "China", "(4) College of Engineering and Computer Science", "Australian National University", "Australia)"], "abstract": "Ultra-high-definition (UHD) images often suffer from severe degradations such as blur, haze, rain, or low-light conditions, which pose significant challenges for image restoration due to their high resolution and computational demands. In this paper, we propose UHDRes, a novel lightweight dual-domain decoupled spectral modulation framework for UHD image restoration. It explicitly models the amplitude spectrum via lightweight spectrum-domain modulation, while restoring phase implicitly through spatial-domain refinement. We introduce the spatio-spectral fusion mechanism, which first employs a multi-scale context aggregator to extract local and global spatial features, and then performs spectral modulation in a decoupled manner. It explicitly enhances amplitude features in the frequency domain while implicitly restoring phase information through spatial refinement. Additionally, a shared gated feed-forward network is designed to efficiently promote feature interaction through shared-parameter convolutions and adaptive gating mechanisms. Extensive experimental comparisons on five public UHD benchmarks demonstrate that our UHDRes achieves the state-of-the-art restoration performance with only 400K parameters, while significantly reducing inference latency and memory usage. The codes and models are available at this https URL .", "primary_subject": "Image and Video Processing (eess.IV)", "subjects": ["Image and Video Processing (eess.IV)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["UHDRes", "spectrum-domain modulation"], "summary": "UHDRes proposes a lightweight dual-domain decoupled spectral modulation framework for high-quality UHD image restoration."}, {"arxiv_id": "arXiv:2511.04892", "title": "LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation", "abs_url": "https://arxiv.org/abs/2511.04892", "pdf_url": "https://arxiv.org/pdf/2511.04892", "authors": ["Vasileios Magoulianitis", "Catherine A. Alexander", "Jiaxin Yang", "C.-C. Jay Kuo"], "abstract": "Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.", "primary_subject": "Image and Video Processing (eess.IV)", "subjects": ["Image and Video Processing (eess.IV)", "Computer Vision and Pattern Recognition (cs.CV)", "Biomolecules (q-bio.BM)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Nuclei segmentation", "self-supervised"], "summary": "LG-NuSegHop proposes a self-supervised pipeline for nuclei segmentation in histology images, achieving competitive performance without manual annotations."}, {"arxiv_id": "arXiv:2511.04834", "title": "Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models", "abs_url": "https://arxiv.org/abs/2511.04834", "pdf_url": "https://arxiv.org/pdf/2511.04834", "authors": ["Jiwoo Shin", "Byeonghu Na", "Mina Kang", "Wonhyeok Choi", "Il-chul Moon"], "abstract": "Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["negative prompts", "concept inversion"], "summary": "This paper proposes using concept inversion to replace negative prompts, enhancing the defense against harmful content in text-to-image generation without modifying existing models."}, {"arxiv_id": "arXiv:2511.04718", "title": "Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification", "abs_url": "https://arxiv.org/abs/2511.04718", "pdf_url": "https://arxiv.org/pdf/2511.04718", "authors": ["Yue Xun", "Jiaxing Xu", "Wenbo Gao", "Chen Yang", "Shujun Wang"], "abstract": "Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations, treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within specific frequency bands, limiting diagnostic sensitivity and specificity. While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be optimal for capturing individual variability or disease-specific alterations. To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each brain region and Frequency-Coupled Connectivity Learning to capture both intra- and nuanced cross-band interactions in a unified functional network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations for diagnostic prediction. Experimental results on the ADNI and ABIDE datasets demonstrate superior performance over existing methods. The code is available at this https URL .", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Adaptive Cascade Decomposition", "Unified-GCN"], "summary": "This paper introduces a novel framework using Adaptive Cascade Decomposition and Unified-GCN to improve brain disorder classification by learning task-relevant frequency sub-bands and capturing nuanced interactions."}, {"arxiv_id": "arXiv:2511.04699", "title": "Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding", "abs_url": "https://arxiv.org/abs/2511.04699", "pdf_url": "https://arxiv.org/pdf/2511.04699", "authors": ["Haneen Al-Homoud", "Asma Ibrahim", "Murtadha Al-Jubran", "Fahad Al-Otaibi", "Yazeed Al-Harbi", "Daulet Toibazar", "Kesen Wang", "Pedro J. Moreno"], "abstract": "Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.", "primary_subject": "Computation and Language (cs.CL)", "subjects": ["Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["SynthDocs", "OCR"], "summary": "SynthDocs, a large synthetic corpus, enhances OCR and document understanding for Arabic by providing extensive annotated data."}], "stats": {"total": 79, "total_authorships": 471, "unique_authors": 465, "section_counts": {"Authors and titles": 79}, "top_authors": [["Han Ding", 2], ["Fei Wang", 2], ["Xinyu Chen", 2], ["Shuo Zhao", 2], ["Jianxu Chen", 2]], "top_phrases": [["anomaly detection", 2], ["Visual Spatial Tuning", 1], ["spatial perception", 1], ["Temporal search", 1], ["Reinforcement learning", 1]], "average_authors": 5.962025316455696}}, "cs.RO": {"label": "Robotics (cs.RO)", "url": "https://arxiv.org/list/cs.RO/recent?skip=0&show=2000", "date": "2025-11-10", "articles": [{"arxiv_id": "arXiv:2511.05426", "title": "Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience", "abs_url": "https://arxiv.org/abs/2511.05426", "pdf_url": "https://arxiv.org/pdf/2511.05426", "authors": ["Luca Girardi", "Gabriel Maquignaz", "Stefano Mintchev"], "abstract": "Natural flyers use soft wings to seamlessly enable a wide range of flight behaviours, including agile manoeuvres, squeezing through narrow passageways, and withstanding collisions. In contrast, conventional quadrotor designs rely on rigid frames that support agile flight but inherently limit collision resilience and squeezability, thereby constraining flight capabilities in cluttered environments. Inspired by the anisotropic stiffness and distributed mass-energy structures observed in biological organisms, we introduce FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off. We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more compliant than conventional quadrotors, yet capable of acrobatic manoeuvres with peak speeds above 80 km/h and linear and angular accelerations exceeding 3 g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate accelerations of rigid counterparts up to a thrust-to-weight ratio of 8. Simultaneously, FlexiQuad exhibits fourfold higher collision resilience, surviving frontal impacts at 5 m/s without damage and reducing destabilising forces in glancing collisions by a factor of 39. Its frame can fully compress, enabling flight through gaps as narrow as 70% of its nominal width. Our analysis identifies an optimal structural softness range, from 0.006 to 0.77 N/mm, comparable to that of natural flyers' wings, whereby agility, squeezability, and collision resilience are jointly achieved for FlexiQuad models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in complex environments, enabling robust physical interactions without compromising flight performance.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["FlexiQuad", "soft-frame quadrotor"], "summary": "Introduces FlexiQuad, a highly compliant yet agile soft-frame quadrotor that enhances collision resilience and squeezability, expanding drone capabilities in cluttered environments."}, {"arxiv_id": "arXiv:2511.05402", "title": "Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications", "abs_url": "https://arxiv.org/abs/2511.05402", "pdf_url": "https://arxiv.org/pdf/2511.05402", "authors": ["Muhammad Saud Ul Hassan", "Derek Vasquez", "Hamza Asif", "Christian Hubicki"], "abstract": "In this paper, we present an energy-conservation based control architecture for stable dynamic motion in quadruped robots. We model the robot as a Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the bouncing motion characteristic of running gaits observed in various biological quadrupeds and bio-inspired robotic systems. The model permits leg-orientation control during flight and leg-length control during stance, a design choice inspired by natural quadruped behaviors and prevalent in robotic quadruped systems. Our control algorithm uses the reduced-order SLIP dynamics of the quadruped to track a stable parabolic spline during stance, which is calculated using the principle of energy conservation. Through simulations based on the design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we demonstrate that our control algorithm generates stable bouncing gaits. Additionally, we illustrate the robustness of our controller by showcasing its ability to maintain stable bouncing even when faced with up to a 10% error in sensor measurements.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["energy conservation", "Spring-loaded Inverted Pendulum"], "summary": "This paper introduces an energy-conservation based control algorithm for quadruped robots, using a Spring-loaded Inverted Pendulum model to achieve stable dynamic motion."}, {"arxiv_id": "arXiv:2511.05397", "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation", "abs_url": "https://arxiv.org/abs/2511.05397", "pdf_url": "https://arxiv.org/pdf/2511.05397", "authors": ["Samarth Chopra", "Alex McMoil", "Ben Carnovale", "Evan Sokolson", "Rajkumar Kubendran", "Samuel Dickerson"], "abstract": "While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: this https URL", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["EverydayVLA", "adaptive-horizon ensemble"], "summary": "EverydayVLA uses a cost-effective manipulator and an adaptive model to achieve state-of-the-art success rates in VLA tasks."}, {"arxiv_id": "arXiv:2511.05379", "title": "ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality", "abs_url": "https://arxiv.org/abs/2511.05379", "pdf_url": "https://arxiv.org/pdf/2511.05379", "authors": ["Eric Godden", "Jacquie Groenewegen", "Matthew K.X.J. Pan"], "abstract": "We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction), a dynamic encountered-type haptic display (ETHD) that enables natural physical contact in virtual reality (VR) during social interactions such as handovers, fist bumps, and high-fives. The system integrates a torque-controlled robotic manipulator with interchangeable passive props (silicone hand replicas and a baton), marker-based physical-virtual registration via a ChArUco board, and a safety monitor that gates motion based on the user's head and hand pose. We introduce two control strategies: (i) a static mode that presents a stationary prop aligned with its virtual counterpart, consistent with prior ETHD baselines, and (ii) a dynamic mode that continuously updates prop position by exponentially blending an initial mid-point trajectory with real-time hand tracking, generating a unique contact point for each interaction. Bench tests show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions achieved temporal alignment with an average contact latency of 28.53 +/- 31.21 ms across all interaction and control conditions. These results demonstrate the feasibility of recreating socially meaningful haptics in VR. By incorporating essential safety and control mechanisms, ETHOS establishes a practical foundation for high-fidelity, dynamic interpersonal interactions in virtual environments.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Systems and Control (eess.SY)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["haptic display", "social interaction"], "summary": "ETHOS introduces a dynamic haptic display for VR, enabling natural physical contact during social interactions like handovers and high-fives."}, {"arxiv_id": "arXiv:2511.05307", "title": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators", "abs_url": "https://arxiv.org/abs/2511.05307", "pdf_url": "https://arxiv.org/pdf/2511.05307", "authors": ["Akua K. Dickson", "Juan C. Pacheco Garcia", "Andrew P. Sabelhaus"], "abstract": "Soft robot manipulators have the potential for deployment in delicate environments to perform complex manipulation tasks. However, existing obstacle detection and avoidance methods do not consider limits on the forces that manipulators may exert upon contact with delicate obstacles. This work introduces a framework that maps force safety criteria from task space (i.e. positions along the robot's body) to configuration space (i.e. the robot's joint angles) and enables real-time force safety detection. We incorporate limits on allowable environmental contact forces for given task-space obstacles, and map them into configuration space (C-space) through the manipulator's forward kinematics. This formulation ensures that configurations classified as safe are provably below the maximum force thresholds, thereby allowing us to determine force-safe configurations of the soft robot manipulator in real-time. We validate our approach in simulation and hardware experiments on a two-segment pneumatic soft robot manipulator. Results demonstrate that the proposed method accurately detects force safety during interactions with deformable obstacles, thereby laying the foundation for real-time safe planning of soft manipulators in delicate, cluttered environments.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Systems and Control (eess.SY)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["force safety", "soft robot"], "summary": "This work presents a framework for real-time force safety detection in soft robot manipulators to ensure safe interactions with delicate obstacles."}, {"arxiv_id": "arXiv:2511.05275", "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models", "abs_url": "https://arxiv.org/abs/2511.05275", "pdf_url": "https://arxiv.org/pdf/2511.05275", "authors": ["Hokyun Im", "Euijin Jeong", "Jianlong Fu", "Andrey Kolobov", "Youngwoon Lee"], "abstract": "Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual tasks. However, because most public datasets focus on single-arm demonstrations, adapting VLAs for bimanual tasks typically requires substantial additional bimanual data and fine-tuning. To address this challenge, we introduce TwinVLA, a modular framework that composes two copies of a pretrained single-arm VLA into a coordinated bimanual VLA. Unlike monolithic cross-embodiment models trained on mixtures of single-arm and bimanual data, TwinVLA improves both data efficiency and performance by composing pretrained single-arm policies. Across diverse bimanual tasks in real-world and simulation settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model without requiring any bimanual pretraining. Furthermore, it narrows the gap to state-of-the-art model, $\\pi_0$ which rely on extensive proprietary bimanual data and compute cost. These results establish our modular composition approach as a data-efficient and scalable path toward high-performance bimanual manipulation, leveraging public single-arm data.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["TwinVLA", "bimanual tasks"], "summary": "TwinVLA, a modular framework, composes two single-arm models to excel in bimanual tasks, improving data efficiency and performance."}, {"arxiv_id": "arXiv:2511.05234", "title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning", "abs_url": "https://arxiv.org/abs/2511.05234", "pdf_url": "https://arxiv.org/pdf/2511.05234", "authors": ["Philipp Dahlinger", "Niklas Freymuth", "Tai Hoang", "Tobias WÃ¼rth", "Michael Volpp", "Luise KÃ¤rger", "Gerhard Neumann"], "abstract": "Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["MeshGraphNet", "Meta-learning"], "summary": "The paper introduces M3GN, a method that uses meta-learning and movement primitives to improve the accuracy and speed of mesh-based simulations compared to existing GNSs."}, {"arxiv_id": "arXiv:2511.05203", "title": "Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space", "abs_url": "https://arxiv.org/abs/2511.05203", "pdf_url": "https://arxiv.org/pdf/2511.05203", "authors": ["Linus Nwankwo", "BjÃ¶rn Ellensohn", "Christian Rauch", "Elmar Rueckert"], "abstract": "Today's autonomous agents can understand free-form natural language instructions and execute long-horizon tasks in a manner akin to human-level reasoning. These capabilities are mostly driven by large-scale pre-trained foundation models (FMs). However, the approaches with which these models are grounded for human-robot interaction (HRI) perpetuate a master-apprentice model, where the apprentice (embodied agent) passively receives and executes the master's (human's) commands without reciprocal learning. This reactive interaction approach does not capture the co-adaptive dynamics inherent in everyday multi-turn human-human interactions. To address this, we propose a Symbiotic Interactive Learning (SIL) approach that enables both the master and the apprentice to co-adapt through mutual, bidirectional interactions. We formalised SIL as a co-adaptation process within a shared latent task space, where the agent and human maintain joint belief states that evolve based on interaction history. This enables the agent to move beyond reactive execution to proactive clarification, adaptive suggestions, and shared plan refinement. To realise these novel behaviours, we leveraged pre-trained FMs for spatial perception and reasoning, alongside a lightweight latent encoder that grounds the models' outputs into task-specific representations. Furthermore, to ensure stability as the tasks evolve, we augment SIL with a memory architecture that prevents the forgetting of learned task-space representations. We validate SIL on both simulated and real-world embodied tasks, including instruction following, information retrieval, query-oriented reasoning, and interactive dialogues. Demos and resources are public at:~\\href{ this https URL }{ this https URL }.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Symbiotic Interactive Learning", "co-adaptive dynamics"], "summary": "The paper introduces SIL, a method for enabling autonomous agents and humans to co-adapt through mutual interactions, enhancing task execution in human-robot interaction."}, {"arxiv_id": "arXiv:2511.05199", "title": "Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation", "abs_url": "https://arxiv.org/abs/2511.05199", "pdf_url": "https://arxiv.org/pdf/2511.05199", "authors": ["Yichen Zhu", "Feifei Feng"], "abstract": "Robots operating in complex and uncertain environments face considerable challenges. Advanced robotic systems often rely on extensive datasets to learn manipulation tasks. In contrast, when humans are faced with unfamiliar tasks, such as assembling a chair, a common approach is to learn by watching video demonstrations. In this paper, we propose a novel method for learning robot policies by Retrieving-from-Video (RfV), using analogies from human demonstrations to address manipulation tasks. Our system constructs a video bank comprising recordings of humans performing diverse daily tasks. To enrich the knowledge from these videos, we extract mid-level information, such as object affordance masks and hand motion trajectories, which serve as additional inputs to enhance the robot model's learning and generalization capabilities. We further feature a dual-component system: a video retriever that taps into an external video bank to fetch task-relevant video based on task specification, and a policy generator that integrates this retrieved knowledge into the learning cycle. This approach enables robots to craft adaptive responses to various scenarios and generalize to tasks beyond those in the training data. Through rigorous testing in multiple simulated and real-world settings, our system demonstrates a marked improvement in performance over conventional robotic systems, showcasing a significant breakthrough in the field of robotics.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Retrieving-from-Video", "human demonstrations"], "summary": "This paper introduces a novel method for robot policy learning using human demonstration videos, enhancing adaptability and generalization in complex tasks."}, {"arxiv_id": "arXiv:2511.05185", "title": "Procedimiento de auditorÃ­a de ciberseguridad para sistemas autÃ³nomos: metodologÃ­a, amenazas y mitigaciones", "abs_url": "https://arxiv.org/abs/2511.05185", "pdf_url": "https://arxiv.org/pdf/2511.05185", "authors": ["AdriÃ¡n Campazas-Vega", "Claudia Ãlvarez-Aparicio", "David SobrÃ­n-Hidalgo", "Laura Inyesto-Alonso", "Francisco Javier RodrÃ­guez-Lera", "Vicente MatellÃ¡n-Olivera", "Ãngel Manuel Guerrero-Higueras"], "abstract": "The deployment of autonomous systems has experienced remarkable growth in recent years, driven by their integration into sectors such as industry, medicine, logistics, and domestic environments. This expansion is accompanied by a series of security issues that entail significant risks due to the critical nature of autonomous systems, especially those operating in human-interaction environments. Furthermore, technological advancement and the high operational and architectural complexity of autonomous systems have resulted in an increased attack surface. This article presents a specific security auditing procedure for autonomous systems, based on a layer-structured methodology, a threat taxonomy adapted to the robotic context, and a set of concrete mitigation measures. The validity of the proposed approach is demonstrated through four practical case studies applied to representative robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1 robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots, and the Pepper social robot from Aldebaran Robotics.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Cryptography and Security (cs.CR)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["security auditing", "autonomous systems"], "summary": "This paper introduces a security auditing procedure for autonomous systems using a layer-structured methodology and threat taxonomy, validated through case studies on various robotic platforms."}, {"arxiv_id": "arXiv:2511.05158", "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning", "abs_url": "https://arxiv.org/abs/2511.05158", "pdf_url": "https://arxiv.org/pdf/2511.05158", "authors": ["Sahar Salimpour", "Iacopo Catalano", "Tomi Westerlund", "Mohsen Falahi", "Jorge PeÃ±a Queralta"], "abstract": "Autonomous micro-mobility platforms face challenges from the perspective of the typical deployment environment: large indoor spaces or urban areas that are potentially crowded and highly dynamic. While social navigation algorithms have progressed significantly, optimizing user comfort and overall user experience over other typical metrics in robotics (e.g., time or distance traveled) is understudied. Specifically, these metrics are critical in commercial applications. In this paper, we show how imitation learning delivers smoother and overall better controllers, versus previously used manually-tuned controllers. We demonstrate how DAAV's autonomous wheelchair achieves state-of-the-art comfort in follow-me mode, in which it follows a human operator assisting persons with reduced mobility (PRM). This paper analyzes different neural network architectures for end-to-end control and demonstrates their usability in real-world production-level deployments.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["imitation learning", "comfort"], "summary": "This paper uses imitation learning to improve the comfort of an autonomous wheelchair in follow-me mode, surpassing manually-tuned controllers."}, {"arxiv_id": "arXiv:2511.05129", "title": "Decomposed Object Manipulation via Dual-Actor Policy", "abs_url": "https://arxiv.org/abs/2511.05129", "pdf_url": "https://arxiv.org/pdf/2511.05129", "authors": ["Bin Fan", "Jianjian Jiang", "Zhuohao Li", "Yixiang He", "Xiaoming Wu", "Yihan Yang", "Shengbang Liu", "Weishi Zheng"], "abstract": "Object manipulation, which focuses on learning to perform tasks on similar parts across different types of objects, can be divided into an approaching stage and a manipulation stage. However, previous works often ignore this characteristic of the task and rely on a single policy to directly learn the whole process of object manipulation. To address this problem, we propose a novel Dual-Actor Policy, termed DAP, which explicitly considers different stages and leverages heterogeneous visual priors to enhance each stage. Specifically, we introduce an affordance-based actor to locate the functional part in the manipulation task, thereby improving the approaching process. Following this, we propose a motion flow-based actor to capture the movement of the component, facilitating the manipulation process. Finally, we introduce a decision maker to determine the current stage of DAP and select the corresponding actor. Moreover, existing object manipulation datasets contain few objects and lack the visual priors needed to support training. To address this, we construct a simulated dataset, the Dual-Prior Object Manipulation Dataset, which combines the two visual priors and includes seven tasks, including two challenging long-term, multi-stage tasks. Experimental results on our dataset, the RoboTwin benchmark and real-world scenarios illustrate that our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4% on average respectively.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Dual-Actor Policy", "Dual-Prior Dataset"], "summary": "The paper introduces DAP, a novel approach for object manipulation that uses two specialized actors and a dual-prior dataset to improve performance across various tasks."}, {"arxiv_id": "arXiv:2511.05052", "title": "TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments", "abs_url": "https://arxiv.org/abs/2511.05052", "pdf_url": "https://arxiv.org/pdf/2511.05052", "authors": ["Zihao Li", "Yiming Zhu", "Zhe Zhong", "Qinyuan Ren", "Yijiang Huang"], "abstract": "Robotic manipulation in complex, constrained spaces is vital for widespread applications but challenging, particularly when navigating narrow passages with elongated objects. Existing planning methods often fail in these low-clearance scenarios due to the sampling difficulties or the local minima. This work proposes Topology-Aware Planning for Object Manipulation (TAPOM), which explicitly incorporates task-space topological analysis to enable efficient planning. TAPOM uses a high-level analysis to identify critical pathways and generate guiding keyframes, which are utilized in a low-level planner to find feasible configuration space trajectories. Experimental validation demonstrates significantly high success rates and improved efficiency over state-of-the-art methods on low-clearance manipulation tasks. This approach offers broad implications for enhancing manipulation capabilities of robots in complex real-world environments.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Systems and Control (eess.SY)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Topology-Aware Planning", "Object Manipulation"], "summary": "TAPOM uses topological analysis to efficiently plan manipulation tasks in narrow spaces, outperforming existing methods."}, {"arxiv_id": "arXiv:2511.05033", "title": "Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems", "abs_url": "https://arxiv.org/abs/2511.05033", "pdf_url": "https://arxiv.org/pdf/2511.05033", "authors": ["Jennifer K. Leestma", "Siddharth R. Nathella", "Christoph P. O. Nuesslein", "Snehil Mathur", "Gregory S. Sawicki", "Aaron J. Young"], "abstract": "Epically Powerful is an open-source robotics infrastructure that streamlines the underlying framework of wearable robotic systems - managing communication protocols, clocking, actuator commands, visualization, sensor data acquisition, data logging, and more - while also providing comprehensive guides for hardware selection, system assembly, and controller implementation. Epically Powerful contains a code base enabling simplified user implementation via Python that seamlessly interfaces with various commercial state-of-the-art quasi-direct drive (QDD) actuators, single-board computers, and common sensors, provides example controllers, and enables real-time visualization. To further support device development, the package also includes a recommended parts list and compatibility guide and detailed documentation on hardware and software implementation. The goal of Epically Powerful is to lower the barrier to developing and deploying custom wearable robotic systems without a pre-specified form factor, enabling researchers to go from raw hardware to modular, robust devices quickly and effectively. Though originally designed with wearable robotics in mind, Epically Powerful is broadly applicable to other robotic domains that utilize QDD actuators, single-board computers, and sensors for closed-loop control.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Epically Powerful", "wearable robotics"], "summary": "Epically Powerful is an open-source infrastructure simplifying the development of wearable robotic systems and applicable to other QDD-based robotic domains."}, {"arxiv_id": "arXiv:2511.05026", "title": "Tunable Passivity Control for Centralized Multiport Networked Systems", "abs_url": "https://arxiv.org/abs/2511.05026", "pdf_url": "https://arxiv.org/pdf/2511.05026", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "abstract": "Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key architecture with applications in several complex network systems, such as multilateral telerobotics and multi-agent control. These systems consist of a hub node/subsystem connecting with multiple remote nodes/subsystems via a networked architecture. One challenge for this system is stability, which can be affected by non-ideal network artifacts. Conventional passivity-based approaches can stabilize the system under specialized applications like small-scale networked systems. However, those conventional passive stabilizers have several restrictions, such as distributing compensation across subsystems in a decentralized manner, limiting flexibility, and, at the same time, relying on the restrictive assumptions of node passivity. This paper synthesizes a centralized optimal passivity-based stabilization framework for CMND systems. It consists of a centralized passivity observer monitoring overall energy flow and an optimal passivity controller that distributes the just-needed dissipation among various nodes, guaranteeing strict passivity and, thus, L2 stability. The proposed data-driven model-free approach, i.e., Tunable Centralized Optimal Passivity Control (TCoPC), optimizes total performance based on the prescribed dissipation distribution strategy while ensuring stability. The controller can put high dissipation loads on some sub-networks while relaxing the dissipation on other nodes. Simulation results demonstrate the proposed frameworks performance in a complex task under different time-varying delay scenarios while relaxing the remote nodes minimum phase and passivity assumption, enhancing the scalability and generalizability.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["CMND systems", "passivity-based stabilization"], "summary": "This paper proposes a centralized optimal passivity-based stabilization framework for CMND systems, enhancing scalability and flexibility compared to conventional methods."}, {"arxiv_id": "arXiv:2511.05007", "title": "MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery", "abs_url": "https://arxiv.org/abs/2511.05007", "pdf_url": "https://arxiv.org/pdf/2511.05007", "authors": ["Baiye Cheng", "Tianhai Liang", "Suning Huang", "Maanping Shao", "Feihong Zhang", "Botian Xu", "Zhengrong Xue", "Huazhe Xu"], "abstract": "Diffusion policies have emerged as a powerful framework for robotic visuomotor control, yet they often lack the robustness to recover from subtask failures in long-horizon, multi-stage tasks and their learned representations of observations are often difficult to interpret. In this work, we propose the Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is to insert a Mixture of Experts (MoE) layer between the visual encoder and the diffusion model. This layer decomposes the policy's knowledge into a set of specialized experts, which are dynamically activated to handle different phases of a task. We demonstrate through extensive experiments that MoE-DP exhibits a strong capability to recover from disturbances, significantly outperforming standard baselines in robustness. On a suite of 6 long-horizon simulation tasks, this leads to a 36% average relative improvement in success rate under disturbed conditions. This enhanced robustness is further validated in the real world, where MoE-DP also shows significant performance gains. We further show that MoE-DP learns an interpretable skill decomposition, where distinct experts correspond to semantic task primitives (e.g., approaching, grasping). This learned structure can be leveraged for inference-time control, allowing for the rearrangement of subtasks without any this http URL video and code are available at the this https URL .", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Mixture of Experts", "Diffusion Policy"], "summary": "The paper introduces MoE-DP, enhancing diffusion policies for robotic tasks by adding a MoE layer, which improves robustness and interpretability."}, {"arxiv_id": "arXiv:2511.04994", "title": "Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems", "abs_url": "https://arxiv.org/abs/2511.04994", "pdf_url": "https://arxiv.org/pdf/2511.04994", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "abstract": "Maintaining system stability and accurate position tracking is imperative in networked robotic systems, particularly for haptics-enabled human-robot interaction. Recent literature has integrated human biomechanics into the stabilizers implemented for teleoperation, enhancing force preservation while guaranteeing convergence and safety. However, position desynchronization due to imperfect communication and non-passive behaviors remains a challenge. This paper proposes a two-port biomechanics-aware passivity-based synchronizer and stabilizer, referred to as TBPS2. This stabilizer optimizes position synchronization by leveraging human biomechanics while reducing the stabilizer's conservatism in its activation. We provide the mathematical design synthesis of the stabilizer and the proof of stability. We also conducted a series of grid simulations and systematic experiments, comparing their performance with that of state-of-the-art solutions under varying time delays and environmental conditions.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["biomechanics-aware", "synchronizer"], "summary": "This paper introduces TBPS2, a stabilizer that enhances position synchronization in networked robotic systems using human biomechanics."}, {"arxiv_id": "arXiv:2511.04992", "title": "A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces", "abs_url": "https://arxiv.org/abs/2511.04992", "pdf_url": "https://arxiv.org/pdf/2511.04992", "authors": ["Bibekananda Patra", "Sandipan Bandyopadhyay"], "abstract": "This article presents a method for computing the largest singularity-free sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a specified orientation workspace. For a fixed orientation of the moving platform, the SFS is computed analytically. This process is repeated over a set of samples generated within the orientation workspace, and the smallest among them is designated as the desired SFS for the given orientation workspace. Numerical experiments are performed on four distinct architectures of the SGPM to understand their relative performances w.r.t. SFS volumes over the same orientation workspace. This study demonstrates the potential utility of the proposed computational method both in analysis and design of SGPMs.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Singularity-free sphere", "Stewart-Gough platform"], "summary": "This paper presents a method to compute the largest singularity-free sphere for a 6-6 Stewart-Gough platform manipulator over a specified orientation workspace."}, {"arxiv_id": "arXiv:2511.04976", "title": "iFlyBot-VLM Technical Report", "abs_url": "https://arxiv.org/abs/2511.04976", "pdf_url": "https://arxiv.org/pdf/2511.04976", "authors": ["Xin Nie", "Zhiyuan Cheng", "Yuan Zhang", "Chao Ji", "Jiajia Wu", "Yuhan Zhang", "Jia Pan"], "abstract": "We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used to improve the domain of Embodied Intelligence. The central objective of iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control. To this end, the model abstracts complex visual and spatial information into a body-agnostic and transferable Operational Language, thereby enabling seamless perception-action closed-loop coordination across diverse robotic platforms. The architecture of iFlyBot-VLM is systematically designed to realize four key functional capabilities essential for embodied intelligence: 1) Spatial Understanding and Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and Control Parameter Generation; 4) Task Planning and Skill Sequencing. We envision iFlyBot-VLM as a scalable and generalizable foundation model for embodied AI, facilitating the progression from specialized task-oriented systems toward generalist, cognitively capable agents. We conducted evaluations on 10 current mainstream embodied intelligence-related VLM benchmark datasets, such as Blink and Where2Place, and achieved optimal performance while preserving the model's general capabilities. We will publicly release both the training data and model weights to foster further research and development in the field of Embodied Intelligence.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Vision-Language Model", "Embodied Intelligence"], "summary": "iFlyBot-VLM enhances embodied intelligence by bridging visual and spatial understanding with robotic action, achieving optimal performance across various benchmarks."}, {"arxiv_id": "arXiv:2511.04837", "title": "Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions", "abs_url": "https://arxiv.org/abs/2511.04837", "pdf_url": "https://arxiv.org/pdf/2511.04837", "authors": ["Cameron Robinson", "Ganghee Jang"], "abstract": "Solar energy is used for many mission-critical applications including space exploration, sensor systems to monitor wildfires, etc. Their operation can be limited or even terminated if solar panels are covered with dust or hit by space debris. To address this issue, we designed panel cleaning mechanisms and tested protective materials. For cleaning mechanisms, we designed and compared a wiper system and a rail system. For protective materials, we found through collision tests that polycarbonate was very promising, though the most important factor was layering a soft material between the panel's surface and a hard material. In the cleaning system comparisons, the wiper-based system was more efficient than the rail-based system in terms of cost, cleaning speed, and total power consumption.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["wiper system", "polycarbonate"], "summary": "The paper compares wiper and rail systems for solar panel cleaning and finds polycarbonate effective as protective material."}, {"arxiv_id": "arXiv:2511.04835", "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning", "abs_url": "https://arxiv.org/abs/2511.04835", "pdf_url": "https://arxiv.org/pdf/2511.04835", "authors": ["Shubham Natraj", "Bruno Sinopoli", "Yiannis Kantaros"], "abstract": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically feasible robot paths. However, their reliance on uniform sampling often leads to poor efficiency and slow planning in complex environments. We introduce a novel non-uniform sampling strategy that integrates into existing SBMPs by biasing sampling toward `certified' regions. These regions are constructed by (i) generating an initial, possibly infeasible, path using any heuristic path predictor (e.g., A* or vision-language models) and (ii) applying conformal prediction to quantify the predictor's uncertainty. This process yields prediction sets around the initial-guess path that are guaranteed, with user-specified probability, to contain the optimal solution. To our knowledge, this is the first non-uniform sampling approach for SBMPs that provides such probabilistically correct guarantees on the sampling regions. Extensive evaluations demonstrate that our method consistently finds feasible paths faster and generalizes better to unseen environments than existing baselines.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["non-uniform sampling", "conformal prediction"], "summary": "The paper introduces a novel non-uniform sampling strategy using conformal prediction to enhance the efficiency and generalization of sampling-based motion planners."}, {"arxiv_id": "arXiv:2511.04831", "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning", "abs_url": "https://arxiv.org/abs/2511.04831", "pdf_url": "https://arxiv.org/pdf/2511.04831", "authors": ["NVIDIA : Mayank Mittal", "Pascal Roth", "James Tigue", "Antoine Richard", "Octi Zhang", "Peter Du", "Antonio Serrano-MuÃ±oz", "Xinjie Yao", "RenÃ© ZurbrÃ¼gg", "Nikita Rudin", "Lukasz Wawrzyniak", "Milad Rakhsha", "Alain Denzler", "Eric Heiden", "Ales Borovicka", "Ossama Ahmed", "Iretiayo Akinola", "Abrar Anwar", "Mark T. Carlson", "Ji Yuan Feng", "Animesh Garg", "Renato Gasoto", "Lionel Gulich", "Yijie Guo", "M. Gussert", "Alex Hansen", "Mihir Kulkarni", "Chenran Li", "Wei Liu", "Viktor Makoviychuk", "Grzegorz Malczyk", "Hammad Mazhar", "Masoud Moghani", "Adithyavairavan Murali", "Michael Noseworthy", "Alexander Poddubny", "Nathan Ratliff", "Welf Rehberg", "Clemens Schwarke", "Ritvik Singh", "James Latham Smith", "Bingjie Tang", "Ruchik Thaker", "Matthew Trepte", "Karl Van Wyk", "Fangzhou Yu", "Alex Millane", "Vikram Ramasamy", "Remo Steiner", "Sangeeta Subramanian", "Clemens Volk", "CY Chen", "Neel Jawale", "Ashwin Varghese Kuruttukulam", "Michael A. Lin", "Ajay Mandlekar", "Karsten Patzwaldt", "John Welsh", "Huihua Zhao", "Fatima Anes", "Jean-Francois Lafleche", "Nicolas MoÃ«nne-Loccoz", "Soowan Park", "Rob Stepinski", "Dirk Van Gelder", "Chris Amevor", "Jan Carius", "Jumyung Chang", "Anka He Chen", "Pablo de Heras Ciechomski", "Gilles Daviet", "Mohammad Mohajerani", "Julia von Muralt", "Viktor Reutskyy", "Michael Sauter", "Simon Schirm", "Eric L. Shi", "Pierre Terdiman", "Kenny Vilella", "Tobias Widmer", "Gordon Yeoman", "Tiffany Chen", "Sergey Grizan", "Cathy Li", "Lotus Li", "Connor Smith", "Rafael Wiltz", "Kostas Alexis", "Yan Chang", "David Chu", "Linxi \"Jim\" Fan", "Farbod Farshidian", "Ankur Handa", "Spencer Huang", "Marco Hutter", "Yashraj Narang", "Soha Pouya", "Shiwei Sheng", "Yuke Zhu"], "abstract": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the paradigm of GPU-native robotics simulation into the era of large-scale multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics, photorealistic rendering, and a modular, composable architecture for designing environments and training robot policies. Beyond physics and rendering, the framework integrates actuator models, multi-frequency sensor simulation, data collection pipelines, and domain randomization tools, unifying best practices for reinforcement and imitation learning at scale within a single extensible platform. We highlight its application to a diverse set of challenges, including whole-body control, cross-embodiment mobility, contact-rich and dexterous manipulation, and the integration of human demonstrations for skill acquisition. Finally, we discuss upcoming integration with the differentiable, GPU-accelerated Newton physics engine, which promises new opportunities for scalable, data-efficient, and gradient-based approaches to robot learning. We believe Isaac Lab's combination of advanced simulation capabilities, rich sensing, and data-center scale execution will help unlock the next generation of breakthroughs in robotics research.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Isaac Lab", "multi-modal learning"], "summary": "Isaac Lab extends robotics simulation for large-scale multi-modal learning, integrating advanced physics, rendering, and sensor simulation."}, {"arxiv_id": "arXiv:2511.04827", "title": "Pixi: Unified Software Development and Distribution for Robotics and AI", "abs_url": "https://arxiv.org/abs/2511.04827", "pdf_url": "https://arxiv.org/pdf/2511.04827", "authors": ["Tobias Fischer", "Wolf Vollprecht", "Bas Zalmstra", "Ruben Arts", "Tim de Jager", "Alejandro Fontan", "Adam D Hines", "Michael Milford", "Silvio Traversaro", "Daniel Claes", "Scarlett Raine"], "abstract": "The reproducibility crisis in scientific computing constrains robotics research. Existing studies reveal that up to 70% of robotics algorithms cannot be reproduced by independent teams, while many others fail to reach deployment because creating shareable software environments remains prohibitively complex. These challenges stem from fragmented, multi-language, and hardware-software toolchains that lead to dependency hell. We present Pixi, a unified package-management framework that addresses these issues by capturing exact dependency states in project-level lockfiles, ensuring bit-for-bit reproducibility across platforms. Its high-performance SAT solver achieves up to 10x faster dependency resolution than comparable tools, while integration of the conda-forge and PyPI ecosystems removes the need for multiple managers. Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours to minutes and lowers technical barriers for researchers worldwide. By enabling scalable, reproducible, collaborative research infrastructure, Pixi accelerates progress in robotics and AI.", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Software Engineering (cs.SE)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["reproducibility", "Pixi"], "summary": "Pixi is a unified package-management framework that enhances reproducibility and accelerates robotics research by simplifying dependency management."}, {"arxiv_id": "arXiv:2511.04812", "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation", "abs_url": "https://arxiv.org/abs/2511.04812", "pdf_url": "https://arxiv.org/pdf/2511.04812", "authors": ["Zixuan Huang", "Huaidian Hou", "Dmitry Berenson"], "abstract": "Given a dataset of expert trajectories, standard imitation learning approaches typically learn a direct mapping from observations (e.g., RGB images) to actions. However, such methods often overlook the rich interplay between different modalities, i.e., sensory inputs, actions, and rewards, which is crucial for modeling robot behavior and understanding task outcomes. In this work, we propose Multimodal Diffusion Forcing, a unified framework for learning from multimodal robot trajectories that extends beyond action generation. Rather than modeling a fixed distribution, MDF applies random partial masking and trains a diffusion model to reconstruct the trajectory. This training objective encourages the model to learn temporal and cross-modal dependencies, such as predicting the effects of actions on force signals or inferring states from partial observations. We evaluate MDF on contact-rich, forceful manipulation tasks in simulated and real-world environments. Our results show that MDF not only delivers versatile functionalities, but also achieves strong performance, and robustness under noisy observations. More visualizations can be found on our website this https URL", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["Multimodal Diffusion Forcing", "diffusion model"], "summary": "This paper introduces Multimodal Diffusion Forcing, a method that learns from robot trajectories by reconstructing masked data, enhancing action prediction and robustness."}, {"arxiv_id": "arXiv:2511.04769", "title": "ReGen: Generative Robot Simulation via Inverse Design", "abs_url": "https://arxiv.org/abs/2511.04769", "pdf_url": "https://arxiv.org/pdf/2511.04769", "authors": ["Phat Nguyen", "Tsun-Hsuan Wang", "Zhang-Wei Hong", "Erfan Aasi", "Andrew Silva", "Guy Rosman", "Sertac Karaman", "Daniela Rus"], "abstract": "Simulation plays a key role in scaling robot learning and validating policies, but constructing simulations remains a labor-intensive process. This paper introduces ReGen, a generative simulation framework that automates simulation design via inverse design. Given a robot's behavior -- such as a motion trajectory or an objective function -- and its textual description, ReGen infers plausible scenarios and environments that could have caused the behavior. ReGen leverages large language models to synthesize scenarios by expanding a directed graph that encodes cause-and-effect relationships, relevant entities, and their properties. This structured graph is then translated into a symbolic program, which configures and executes a robot simulation environment. Our framework supports (i) augmenting simulations based on ego-agent behaviors, (ii) controllable, counterfactual scenario generation, (iii) reasoning about agent cognition and mental states, and (iv) reasoning with distinct sensing modalities, such as braking due to faulty GPS signals. We demonstrate ReGen in autonomous driving and robot manipulation tasks, generating more diverse, complex simulated environments compared to existing simulations with high success rates, and enabling controllable generation for corner cases. This approach enhances the validation of robot policies and supports data or simulation augmentation, advancing scalable robot learning for improved generalization and robustness. We provide code and example videos at: this https URL", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["ReGen", "inverse design"], "summary": "ReGen uses inverse design and large language models to automate simulation creation for robots, enhancing policy validation and generalization."}, {"arxiv_id": "arXiv:2511.04758", "title": "ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling", "abs_url": "https://arxiv.org/abs/2511.04758", "pdf_url": "https://arxiv.org/pdf/2511.04758", "authors": ["Caelan Garrett", "Fabio Ramos"], "abstract": "Bimanual and humanoid robots are appealing because of their human-like ability to leverage multiple arms to efficiently complete tasks. However, controlling multiple arms at once is computationally challenging due to the growth in the hybrid discrete-continuous action space. Task and Motion Planning (TAMP) algorithms can efficiently plan in hybrid spaces but generally produce plans, where only one arm is moving at a time, rather than schedules that allow for parallel arm motion. In order to extend TAMP to produce schedules, we present ScheduleStream, the first general-purpose framework for planning & scheduling with sampling operations. ScheduleStream models temporal dynamics using hybrid durative actions, which can be started asynchronously and persist for a duration that's a function of their parameters. We propose domain-independent algorithms that solve ScheduleStream problems without any application-specific mechanisms. We apply ScheduleStream to Task and Motion Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers to expedite planning. We compare ScheduleStream algorithms to several ablations in simulation and find that they produce more efficient solutions. We demonstrate ScheduleStream on several real-world bimanual robot tasks at this https URL .", "primary_subject": "Robotics (cs.RO)", "subjects": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["ScheduleStream", "TAMPAS"], "summary": "This paper introduces ScheduleStream, a framework for planning and scheduling with multiple arms in Task and Motion Planning & Scheduling problems."}, {"arxiv_id": "arXiv:2511.05396", "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction", "abs_url": "https://arxiv.org/abs/2511.05396", "pdf_url": "https://arxiv.org/pdf/2511.05396", "authors": ["Yiting He", "Zhishuai Liu", "Weixin Wang", "Pan Xu"], "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)", "Machine Learning (stat.ML)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["online RMDP", "supremal visitation ratio"], "summary": "This paper introduces the supremal visitation ratio to measure exploration difficulty in online RMDPs and proposes an algorithm achieving optimal sublinear regret."}, {"arxiv_id": "arXiv:2511.05355", "title": "SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning", "abs_url": "https://arxiv.org/abs/2511.05355", "pdf_url": "https://arxiv.org/pdf/2511.05355", "authors": ["Tzu-Yuan Huang", "Armin Lederer", "Dai-Jie Wu", "Xiaobing Dai", "Sihua Zhang", "Stefan Sosnowski", "Shao-Hua Sun", "Sandra Hirche"], "abstract": "Flow matching (FM) has shown promising results in data-driven planning. However, it inherently lacks formal guarantees for ensuring state and action constraints, whose satisfaction is a fundamental and crucial requirement for the safety and admissibility of planned trajectories on various systems. Moreover, existing FM planners do not ensure the dynamical consistency, which potentially renders trajectories inexecutable. We address these shortcomings by proposing SAD-Flower, a novel framework for generating Safe, Admissible, and Dynamically consistent trajectories. Our approach relies on an augmentation of the flow with a virtual control input. Thereby, principled guidance can be derived using techniques from nonlinear control theory, providing formal guarantees for state constraints, action constraints, and dynamic consistency. Crucially, SAD-Flower operates without retraining, enabling test-time satisfaction of unseen constraints. Through extensive experiments across several tasks, we demonstrate that SAD-Flower outperforms various generative-model-based baselines in ensuring constraint satisfaction.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Robotics (cs.RO)", "Systems and Control (eess.SY)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["SAD-Flower", "flow matching"], "summary": "SAD-Flower enhances flow matching by ensuring safety, admissibility, and dynamical consistency without retraining."}, {"arxiv_id": "arXiv:2511.05311", "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "abs_url": "https://arxiv.org/abs/2511.05311", "pdf_url": "https://arxiv.org/pdf/2511.05311", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "RaphaÃ«l Frank"], "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.", "primary_subject": "Artificial Intelligence (cs.AI)", "subjects": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Robotics (cs.RO)", "Software Engineering (cs.SE)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["large language models", "predictive maintenance"], "summary": "The study explores how large language models can clean maintenance logs, supporting predictive maintenance in the automotive sector."}, {"arxiv_id": "arXiv:2511.05005", "title": "Multi-agent Coordination via Flow Matching", "abs_url": "https://arxiv.org/abs/2511.05005", "pdf_url": "https://arxiv.org/pdf/2511.05005", "authors": ["Dongsu Lee", "Daehee Lee", "Amy Zhang"], "abstract": "This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\\boldsymbol{\\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.", "primary_subject": "Machine Learning (cs.LG)", "subjects": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "section_type": "Authors and titles", "submission_date": "2025-11-10", "keywords": ["MAC-Flow", "multi-agent coordination"], "summary": "MAC-Flow framework improves multi-agent coordination by balancing complex behavior representation with fast execution, outperforming previous methods."}], "stats": {"total": 30, "total_authorships": 238, "unique_authors": 235, "section_counts": {"Authors and titles": 30}, "top_authors": [["Xingyuan Zhou", 2], ["Peter Paik", 2], ["S. Farokh Atashzar", 2], ["Luca Girardi", 1], ["Gabriel Maquignaz", 1]], "top_phrases": [["FlexiQuad", 1], ["soft-frame quadrotor", 1], ["energy conservation", 1], ["Spring-loaded Inverted Pendulum", 1], ["EverydayVLA", 1]], "average_authors": 7.933333333333334}}}, "preferences": {"favorite_authors": ["barron", "Fei-Fei Li"], "keywords": ["reconstruction", "gaussian splatting"]}, "default_source": "cs.CV"}</script>
  <script>


(() => {
  const RAW_DATA = JSON.parse(document.getElementById('digest-data').textContent);
  const SOURCE_STORAGE_KEY = 'arxivDigestSource';
  const PREF_STORAGE_KEY = 'arxivDigestPreferences';
  const DISPLAY_MODE_STORAGE_KEY = 'arxivDigestDisplayMode';
  const DISPLAY_MODE_CLASSES = {
    title: 'display-mode-title',
    authors: 'display-mode-authors',
    full: 'display-mode-full',
  };
  const DISPLAY_MODE_OPTIONS = [
    { key: 'title', label: 'Title only' },
    { key: 'authors', label: 'Title & authors' },
    { key: 'full', label: 'Full details' },
  ];
  let modalHandlersBound = false;

  const SOURCE_KEYS = Object.keys(RAW_DATA.sources || {});
  if (!SOURCE_KEYS.length) {
    return;
  }
  const generatedAt = RAW_DATA.generated_at || '';
  const initialPreferences = normalizePreferences(RAW_DATA.preferences || {});

  const state = {
    source: loadStoredSource(),
    preferences: loadStoredPreferences(),
    isEditingPreferences: false,
    activeSection: 'stats',
    displayMode: loadStoredDisplayMode() || 'authors',
    expandedArticles: new Set(),
    lastModalTrigger: null,
  };

  if (!RAW_DATA.sources[state.source]) {
    state.source = RAW_DATA.default_source && RAW_DATA.sources[RAW_DATA.default_source]
      ? RAW_DATA.default_source
      : SOURCE_KEYS[0];
  }

  const elements = {
    sourceSwitcher: document.getElementById('source-switcher'),
    displayModeControls: document.getElementById('display-mode-controls'),
    nav: document.querySelector('.sidebar nav'),
    preferencesView: document.getElementById('preferences-view'),
    preferencesForm: document.getElementById('preferences-form'),
    favoriteAuthorsView: document.getElementById('favorite-authors-view'),
    keywordsView: document.getElementById('keywords-view'),
    favoritesInput: document.getElementById('favorite-authors-input'),
    keywordsInput: document.getElementById('keywords-input'),
    editPreferences: document.getElementById('edit-preferences'),
    cancelPreferences: document.getElementById('cancel-preferences'),
    resetPreferences: document.getElementById('reset-preferences'),
    preferencesStatusView: document.getElementById('preferences-status-view'),
    preferencesStatus: document.getElementById('preferences-status'),
    overviewSummary: document.getElementById('overview-summary'),
    overviewBody: document.getElementById('overview-body'),
    statsBody: document.getElementById('stats-body'),
    favoritesBody: document.getElementById('favorite-body'),
    keywordsBody: document.getElementById('keywords-body'),
    categoriesBody: document.getElementById('categories-body'),
    headerSource: document.getElementById('meta-source'),
    headerDate: document.getElementById('meta-date'),
    headerGenerated: document.getElementById('meta-generated'),
    headerTotal: document.getElementById('meta-total'),
    footerSource: document.getElementById('footer-source'),
    abstractModal: document.getElementById('abstract-modal'),
    abstractModalClose: document.getElementById('abstract-modal-close'),
    abstractModalTitle: document.getElementById('abstract-modal-title'),
    abstractModalBody: document.getElementById('abstract-modal-body'),
    abstractModalId: document.getElementById('abstract-modal-id'),
    abstractModalAuthors: document.getElementById('abstract-modal-authors'),
    abstractModalSubjects: document.getElementById('abstract-modal-subjects'),
    abstractModalSummary: document.getElementById('abstract-modal-summary'),
    abstractModalAbstract: document.getElementById('abstract-modal-abstract'),
    abstractModalOriginal: document.getElementById('abstract-modal-original'),
    abstractModalPdf: document.getElementById('abstract-modal-pdf'),
  };

  document.addEventListener('click', handleQuickViewClick);
  document.addEventListener('click', handlePaperClick);
  document.addEventListener('keydown', handlePaperKeydown);
  document.addEventListener('keydown', handleGlobalKeydown);

  if (elements.editPreferences) {
    elements.editPreferences.addEventListener('click', () => {
      state.isEditingPreferences = true;
      setStatus('');
      renderPreferencesPanel();
      if (elements.favoritesInput) {
        elements.favoritesInput.focus();
      }
    });
  }

  if (elements.cancelPreferences) {
    elements.cancelPreferences.addEventListener('click', () => {
      state.isEditingPreferences = false;
      setStatus('');
      renderPreferencesPanel();
    });
  }

  if (elements.preferencesForm) {
    elements.preferencesForm.addEventListener('submit', (event) => {
      event.preventDefault();
      const nextPrefs = normalizePreferences({
        favorite_authors: elements.favoritesInput ? elements.favoritesInput.value : '',
        keywords: elements.keywordsInput ? elements.keywordsInput.value : '',
      });
      state.preferences = nextPrefs;
      state.isEditingPreferences = false;
      savePreferences(nextPrefs);
      renderAll({ resetActiveSection: false });
      setStatus('Preferences saved.');
    });
  }

  if (elements.resetPreferences) {
    elements.resetPreferences.addEventListener('click', () => {
      state.preferences = normalizePreferences(initialPreferences);
      state.isEditingPreferences = true;
      savePreferences(state.preferences);
      renderAll({ resetActiveSection: false });
      setStatus('Preferences reset to defaults.');
    });
  }

  renderAll({ resetActiveSection: true });

  function renderAll(options = {}) {
    if (options.resetActiveSection) {
      state.activeSection = 'stats';
    }
    renderSourceButtons();
    renderDisplayModeControls();
    renderPreferencesPanel();

    const sourceData = RAW_DATA.sources[state.source];
    if (!sourceData) {
      return;
    }

    const articles = sourceData.articles || [];
    pruneExpandedArticles(articles);
    applyDisplayModeClass();
    updateHeader(sourceData);
    const overviewCount = renderOverview(sourceData, articles);
    renderStats(sourceData);
    const favoriteCount = renderFavorites(sourceData, articles);
    const keywordCount = renderKeywords(sourceData, articles);
    const categoriesNavItems = renderCategories(sourceData, articles);
    renderNavigation(sourceData, overviewCount, favoriteCount, keywordCount, categoriesNavItems);
    updateFooter(sourceData);
    attachSectionHandlers();
    attachModalHandlers();
    setActiveSection(state.activeSection);
    updatePaperAria();
  }

  function renderSourceButtons() {
    const container = elements.sourceSwitcher;
    if (!container) return;
    container.innerHTML = SOURCE_KEYS.map((key) => {
      const label = RAW_DATA.sources[key].label || key;
      const active = key === state.source ? 'is-active' : '';
      return `<button type="button" class="source-button ${active}" data-source="${key}">${escapeHtml(label)}</button>`;
    }).join('');
    Array.from(container.querySelectorAll('button[data-source]')).forEach((button) => {
      button.addEventListener('click', () => {
        const nextSource = button.getAttribute('data-source');
        if (!nextSource || nextSource === state.source || !RAW_DATA.sources[nextSource]) return;
        state.source = nextSource;
        saveSource(nextSource);
        setStatus('');
        state.activeSection = 'stats';
        state.expandedArticles.clear();
        renderAll({ resetActiveSection: true });
      });
    });
  }

  function renderDisplayModeControls() {
    const container = elements.displayModeControls;
    if (!container) return;
    const label = `<span class="display-mode__label">Paper view</span>`;
    const buttons = DISPLAY_MODE_OPTIONS.map(({ key, label: buttonLabel }) => {
      return `<button type="button" class="display-mode__button" data-mode="${key}" aria-pressed="false">${escapeHtml(buttonLabel)}</button>`;
    }).join('');
    container.innerHTML = `${label}${buttons}`;
    Array.from(container.querySelectorAll('button[data-mode]')).forEach((button) => {
      button.addEventListener('click', () => {
        const mode = button.getAttribute('data-mode');
        if (!mode) return;
        setDisplayMode(mode);
      });
    });
    updateDisplayModeButtons();
  }

  function setDisplayMode(mode) {
    const normalized = normalizeDisplayMode(mode);
    if (normalized === state.displayMode) {
      return;
    }
    state.displayMode = normalized;
    saveDisplayMode(normalized);
    applyDisplayModeClass();
    updateDisplayModeButtons();
    updatePaperAria();
  }

  function applyDisplayModeClass() {
    const targetClass = DISPLAY_MODE_CLASSES[state.displayMode] || DISPLAY_MODE_CLASSES.full;
    const classList = document.body.classList;
    Object.values(DISPLAY_MODE_CLASSES).forEach((cls) => classList.remove(cls));
    classList.add(targetClass);
  }

  function updateDisplayModeButtons() {
    const container = elements.displayModeControls;
    if (!container) return;
    const activeMode = state.displayMode;
    Array.from(container.querySelectorAll('button[data-mode]')).forEach((button) => {
      const mode = button.getAttribute('data-mode');
      const isActive = mode === activeMode;
      button.classList.toggle('is-active', isActive);
      button.setAttribute('aria-pressed', String(isActive));
    });
  }

  function renderPreferencesPanel() {
    const favorites = state.preferences.favorite_authors || [];
    const keywords = state.preferences.keywords || [];

    if (elements.favoriteAuthorsView) {
      elements.favoriteAuthorsView.innerHTML = favorites.length
        ? favorites.map((item) => `<span class="chip">${escapeHtml(item)}</span>`).join('')
        : '<span class="preferences-empty">None</span>';
    }
    if (elements.keywordsView) {
      elements.keywordsView.innerHTML = keywords.length
        ? keywords.map((item) => `<span class="chip">${escapeHtml(item)}</span>`).join('')
        : '<span class="preferences-empty">None</span>';
    }

    if (state.isEditingPreferences) {
      if (elements.preferencesView) elements.preferencesView.hidden = true;
      if (elements.preferencesForm) elements.preferencesForm.hidden = false;
      updatePreferenceInputs();
    } else {
      if (elements.preferencesView) elements.preferencesView.hidden = false;
      if (elements.preferencesForm) elements.preferencesForm.hidden = true;
    }
  }

  function renderOverview(sourceData, articles) {
    const body = elements.overviewBody;
    if (!body) return 0;
    const summary = elements.overviewSummary;
    const total = articles.length;
    const sourceLabel = sourceData.label || state.source;
    const plural = total === 1 ? '' : 's';
    if (summary) {
      summary.textContent = total + ' paper' + plural + ' from ' + sourceLabel + '.';
    }
    body.innerHTML = articles.map(renderArticleCard).join('') || '<p class="empty-state">No papers available.</p>';
    return total;
  }

  function renderStats(sourceData) {
    const body = elements.statsBody;
    if (!body) return;
    const stats = sourceData.stats || {};
    const total = stats.total || 0;
    const uniqueAuthors = stats.unique_authors || 0;
    const totalAuthorships = stats.total_authorships || 0;
    const averageAuthors = (stats.average_authors || 0).toFixed(2);
    const topAuthors = (stats.top_authors || []).map(([name, count]) => `<li>${escapeHtml(name)} (${count})</li>`).join('') || '<li>None</li>';
    const topKeywords = (stats.top_phrases || []).map(([phrase, count]) => `<li>${escapeHtml(phrase)} (${count})</li>`).join('') || '<li>None</li>';
    const sectionCounts = Object.entries(stats.section_counts || {})
      .sort(([a], [b]) => a.localeCompare(b))
      .map(([section, count]) => `<li>${escapeHtml(section)} (${count})</li>`)
      .join('') || '<li>None</li>';

    body.innerHTML = `
      <div class="stats-grid">
        <div class="stat-card">
          <h3>Papers</h3>
          <p>Total papers: ${total}</p>
          <p>Avg authors per paper: ${averageAuthors}</p>
        </div>
        <div class="stat-card">
          <h3>Authors</h3>
          <p>Unique authors: ${uniqueAuthors}</p>
          <p>Total author mentions: ${totalAuthorships}</p>
        </div>
        <div class="stat-card">
          <h3>Top Authors</h3>
          <ul>${topAuthors}</ul>
        </div>
        <div class="stat-card">
          <h3>Popular Keywords</h3>
          <ul>${topKeywords}</ul>
        </div>
        <div class="stat-card">
          <h3>Section Breakdown</h3>
          <ul>${sectionCounts}</ul>
        </div>
      </div>
    `;
  }

  function renderFavorites(sourceData, articles) {
    const body = elements.favoritesBody;
    if (!body) return 0;
    const favorites = state.preferences.favorite_authors || [];
    const matches = filterByFavoriteAuthors(articles, favorites);
    body.innerHTML = buildWatcherSectionContent(favorites, matches, 'Add authors in the sidebar to highlight researchers you care about.');
    return matches.length;
  }

  function renderKeywords(sourceData, articles) {
    const body = elements.keywordsBody;
    if (!body) return 0;
    const keywords = state.preferences.keywords || [];
    const matches = filterByKeywords(articles, keywords);
    body.innerHTML = buildWatcherSectionContent(keywords, matches, 'Track important topics by adding keywords in the sidebar.');
    return matches.length;
  }

  function buildWatcherSectionContent(items, matches, emptyMessage) {
    const chips = (items || []).map((item) => `<span class="chip">${escapeHtml(item)}</span>`).join('');
    const summary = items.length
      ? `<div class="watcher-summary">Watching <strong>${items.length}</strong> entr${items.length === 1 ? 'y' : 'ies'}.<div class="chip-set">${chips}</div></div>`
      : `<div class="watcher-summary">${emptyMessage}</div>`;
    const articlesHtml = matches.length
      ? matches.map(renderArticleCard).join('')
      : '<p class="empty-state">No papers matched the current filters.</p>';
    return `${summary}${articlesHtml}`;
  }

  function renderCategories(sourceData, articles) {
    const body = elements.categoriesBody;
    if (!body) return [];
    const groups = buildSectionGrouping(articles);
    if (!groups.length) {
      body.innerHTML = '<p class="empty-state">No categories available for this source.</p>';
      return [];
    }
    const sectionsHtml = groups.map(({ sectionId, sectionLabel, count, subjects }) => {
      const subjectHtml = subjects.map(({ subjectId, subjectLabel, items }) => `
        <div class="subject-group" id="${subjectId}">
          <div class="subject-group__header">
            <h4>${escapeHtml(subjectLabel)}</h4>
            <span class="count-chip">${formatCount(items.length)}</span>
          </div>
          ${items.map(renderArticleCard).join('')}
        </div>
      `).join('');
      return `
        <div class="category-block" id="${sectionId}">
          <div class="category-block__header">
            <h3>${escapeHtml(sectionLabel)}</h3>
            <span class="count-chip">${formatCount(count)}</span>
          </div>
          <div class="subject-grid">
            ${subjectHtml}
          </div>
        </div>
      `;
    }).join('');
    body.innerHTML = sectionsHtml;
    return groups.map(({ sectionId, sectionLabel, count, subjects }) => ({
      id: sectionId,
      label: `${sectionLabel} (${count})`,
      children: subjects.map(({ subjectId, subjectLabel, items }) => ({
        id: subjectId,
        label: `${subjectLabel} (${items.length})`,
      })),
    }));
  }

  function renderNavigation(sourceData, overviewCount, favoriteCount, keywordCount, categoriesNavItems) {
    if (!elements.nav) return;
    const navItems = [
      { id: 'stats', label: 'Statistics' },
      { id: 'overview', label: `All Papers (${overviewCount})` },
      { id: 'favorite', label: `Favorite Authors (${favoriteCount})` },
      { id: 'keyword', label: `Watched Keywords (${keywordCount})` },
      { id: 'categories', label: 'Browse by Category', children: categoriesNavItems },
    ];
    elements.nav.innerHTML = buildNavList(navItems);
  }

  function buildNavList(items, level = 1) {
    if (!items || !items.length) return '';
    const listClass = `nav-list nav-level-${level}`;
    const inner = items.map((item) => {
      const children = buildNavList(item.children || [], level + 1);
      return `<li class="nav-item nav-level-${level}"><a href="#${item.id}">${escapeHtml(item.label)}</a>${children}</li>`;
    }).join('');
    return `<ul class="${listClass}">${inner}</ul>`;
  }

  function updateHeader(sourceData) {
    if (elements.headerSource) elements.headerSource.textContent = `Source: ${sourceData.label || state.source}`;
    if (elements.headerDate) elements.headerDate.textContent = `Date: ${sourceData.date || ''}`;
    if (elements.headerGenerated) elements.headerGenerated.textContent = `Generated at: ${generatedAt}`;
    if (elements.headerTotal) elements.headerTotal.textContent = `Total papers: ${(sourceData.stats && sourceData.stats.total) || 0}`;
  }

  function updateFooter(sourceData) {
    if (!elements.footerSource) return;
    elements.footerSource.textContent = sourceData.label || state.source;
    if (sourceData.url) {
      elements.footerSource.setAttribute('href', sourceData.url);
    }
  }

  function attachSectionHandlers() {
    const toggles = Array.from(document.querySelectorAll('.section-toggle'));
    toggles.forEach((toggle) => {
      toggle.onclick = () => {
        const targetId = toggle.getAttribute('data-target');
        if (!targetId) return;
        const section = document.getElementById(targetId);
        if (!section) return;
        const willExpand = section.classList.contains('is-collapsed');
        setSectionState(section, willExpand);
        if (willExpand) {
          state.activeSection = section.id;
          setActiveSection(section.id);
        }
      };
    });
    const navLinks = elements.nav ? Array.from(elements.nav.querySelectorAll('a[href^="#"]')) : [];
    navLinks.forEach((link) => {
      link.onclick = (event) => {
        const href = link.getAttribute('href');
        if (!href || !href.startsWith('#')) return;
        const targetId = href.slice(1);
        const targetElement = document.getElementById(targetId);
        if (!targetElement) return;
        const container = targetElement.classList.contains('content-section')
          ? targetElement
          : targetElement.closest('.content-section');
        if (!container) return;
        event.preventDefault();
        state.activeSection = container.id;
        setActiveSection(container.id, targetElement);
      };
    });
  }

  function setActiveSection(sectionId, focusTarget) {
    state.activeSection = sectionId || 'stats';
    const sections = Array.from(document.querySelectorAll('.content-section'));
    sections.forEach((section) => {
      const isActive = section.id === state.activeSection;
      section.classList.toggle('is-hidden', !isActive);
      if (section.dataset.collapsible === 'true') {
        setSectionState(section, isActive);
      } else {
        section.classList.toggle('is-collapsed', !isActive);
      }
    });
    const activeSection = document.getElementById(state.activeSection);
    if (activeSection) {
      const scrollTarget = focusTarget && activeSection.contains(focusTarget) ? focusTarget : activeSection;
      expandAncestors(scrollTarget);
      scrollTarget.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }
    if (elements.nav) {
      const navLinks = Array.from(elements.nav.querySelectorAll('a[href^="#"]'));
      navLinks.forEach((link) => {
        const href = link.getAttribute('href');
        const id = href ? href.slice(1) : '';
        link.classList.toggle('is-active', id === state.activeSection);
      });
    }
  }

  function updatePaperAria() {
    const cards = Array.from(document.querySelectorAll('.paper'));
    cards.forEach((card) => {
      const paperId = card.getAttribute('data-paper-id') || '';
      const isUserExpanded = paperId && state.expandedArticles.has(paperId);
      const isExpanded = state.displayMode === 'full' || Boolean(isUserExpanded);
      card.setAttribute('aria-expanded', String(isExpanded));
      card.classList.toggle('paper--expanded', Boolean(isUserExpanded));
    });
  }

  function handleQuickViewClick(event) {
    const button = event.target.closest('.js-view-abstract');
    if (!button) return;
    const articleId = button.getAttribute('data-article-id') || '';
    const details = buildModalDetails(articleId, button);
    if (!details.url && !details.title) return;
    event.preventDefault();
    event.stopPropagation();
    openAbstractModal(details, button);
  }

  function handlePaperClick(event) {
    const paper = event.target.closest('.paper');
    if (!paper) return;
    if (event.target.closest('a')) return;
    if (event.target.closest('.js-view-abstract')) return;
    togglePaperExpansion(paper);
  }

  function handlePaperKeydown(event) {
    if (event.key !== 'Enter' && event.key !== ' ') return;
    const paper = event.target.closest('.paper');
    if (!paper) return;
    if (event.key === ' ') {
      event.preventDefault();
    }
    togglePaperExpansion(paper);
  }

  function togglePaperExpansion(paper) {
    const paperId = paper.getAttribute('data-paper-id') || '';
    if (!paperId) return;
    if (state.expandedArticles.has(paperId)) {
      state.expandedArticles.delete(paperId);
    } else {
      state.expandedArticles.add(paperId);
    }
    updatePaperAria();
  }

  function attachModalHandlers() {
    if (modalHandlersBound) return;
    modalHandlersBound = true;
    if (elements.abstractModalClose) {
      elements.abstractModalClose.addEventListener('click', () => closeAbstractModal());
    }
    if (elements.abstractModal) {
      elements.abstractModal.addEventListener('click', (event) => {
        if (event.target && event.target.getAttribute('data-modal-dismiss') === 'true') {
          closeAbstractModal();
        }
      });
    }
  }

  function handleGlobalKeydown(event) {
    if (event.key === 'Escape' && isModalOpen()) {
      event.preventDefault();
      closeAbstractModal();
    }
  }

  function isModalOpen() {
    return Boolean(elements.abstractModal && elements.abstractModal.classList.contains('is-open'));
  }

  function openAbstractModal(details, trigger) {
    if (!elements.abstractModal) return;
    state.lastModalTrigger = trigger || null;
    const resolvedTitle = decodeHtml(details.title) || 'Preview abstract';
    const resolvedUrl = decodeHtml(details.url);
    const resolvedAuthors = decodeHtml(details.authors);
    const resolvedSubjects = decodeHtml(details.subjects);
    const resolvedSummary = decodeHtml(details.summary);
    const resolvedAbstract = decodeHtml(details.abstract);
    const resolvedId = decodeHtml(details.arxivId);
    const resolvedPdf = decodeHtml(details.pdfUrl);
    elements.abstractModal.classList.add('is-open');
    elements.abstractModal.setAttribute('aria-hidden', 'false');
    document.body.classList.add('modal-open');
    if (elements.abstractModalClose) {
      elements.abstractModalClose.focus();
    }
    if (elements.abstractModalTitle) {
      elements.abstractModalTitle.textContent = resolvedTitle;
    }
    if (elements.abstractModalBody) {
      elements.abstractModalBody.scrollTop = 0;
    }
    if (elements.abstractModalId) {
      elements.abstractModalId.textContent = resolvedId ? `ID: ${resolvedId}` : '';
      elements.abstractModalId.hidden = !resolvedId;
    }
    if (elements.abstractModalAuthors) {
      elements.abstractModalAuthors.textContent = resolvedAuthors ? `Authors: ${resolvedAuthors}` : '';
      elements.abstractModalAuthors.hidden = !resolvedAuthors;
    }
    if (elements.abstractModalSubjects) {
      elements.abstractModalSubjects.textContent = resolvedSubjects ? `Subjects: ${resolvedSubjects}` : '';
      elements.abstractModalSubjects.hidden = !resolvedSubjects;
    }
    if (elements.abstractModalSummary) {
      elements.abstractModalSummary.textContent = resolvedSummary ? `Summary: ${resolvedSummary}` : '';
      elements.abstractModalSummary.hidden = !resolvedSummary;
    }
    if (elements.abstractModalAbstract) {
      elements.abstractModalAbstract.textContent = resolvedAbstract || 'No abstract available.';
    }
    if (elements.abstractModalOriginal) {
      if (resolvedUrl) {
        elements.abstractModalOriginal.href = resolvedUrl;
        elements.abstractModalOriginal.hidden = false;
      } else {
        elements.abstractModalOriginal.href = '#';
        elements.abstractModalOriginal.hidden = true;
      }
    }
    if (elements.abstractModalPdf) {
      if (resolvedPdf) {
        elements.abstractModalPdf.href = resolvedPdf;
        elements.abstractModalPdf.hidden = false;
      } else {
        elements.abstractModalPdf.href = '#';
        elements.abstractModalPdf.hidden = true;
      }
    }
  }

  function closeAbstractModal() {
    if (!elements.abstractModal) return;
    elements.abstractModal.classList.remove('is-open');
    elements.abstractModal.setAttribute('aria-hidden', 'true');
    document.body.classList.remove('modal-open');
    if (state.lastModalTrigger && typeof state.lastModalTrigger.focus === 'function') {
      state.lastModalTrigger.focus();
    }
    state.lastModalTrigger = null;
  }

  function setSectionState(section, expanded) {
    if (expanded) {
      section.classList.add('is-expanded');
      section.classList.remove('is-collapsed');
    } else {
      section.classList.add('is-collapsed');
      section.classList.remove('is-expanded');
    }
    const toggle = section.querySelector('.section-toggle');
    if (toggle) {
      toggle.setAttribute('aria-expanded', String(expanded));
      toggle.textContent = expanded ? 'Hide section' : 'Show section';
    }
  }

  function expandAncestors(element) {
    if (!element) return;
    let parent = element.closest('[data-collapsible="true"]');
    while (parent) {
      setSectionState(parent, true);
      parent = parent.parentElement ? parent.parentElement.closest('[data-collapsible="true"]') : null;
    }
  }

  function renderArticleCard(article) {
    const articleId = String(article.arxiv_id || article.id || '');
    const title = escapeHtml(article.title);
    const authors = escapeHtml((article.authors || []).join(', '));
    const subjects = escapeHtml((article.subjects || []).join('; '));
    const abstract = escapeHtml(article.abstract);
    const summary = escapeHtml(article.summary || '');
    const absUrl = escapeHtml(article.abs_url);
    const pdfUrl = article.pdf_url ? escapeHtml(article.pdf_url) : '';
    const pdfLink = pdfUrl ? `<a href="${pdfUrl}" target="_blank" rel="noopener">PDF</a>` : '';
    const quickViewButton = `<button type="button" class="link-button quick-view-button js-view-abstract" data-abs-url="${absUrl}" data-article-title="${title}" data-article-authors="${authors}" data-article-subjects="${subjects}" data-article-abstract="${abstract}" data-article-summary="${summary}" data-article-id="${escapeHtml(article.arxiv_id)}" data-article-pdf="${pdfUrl}">Quick view</button>`;
    const keywords = Array.isArray(article.keywords) ? article.keywords : [];
    const keywordBadges = keywords.length
      ? `<span class="keyword-tags">${keywords.map((keyword) => `<span class="keyword-tag">${escapeHtml(keyword)}</span>`).join('')}</span>`
      : '';
    const linkItems = [`<a href="${absUrl}" target="_blank" rel="noopener">Abstract</a>`, pdfLink].filter(Boolean).join(' ');
    const isUserExpanded = state.expandedArticles.has(articleId);
    const ariaExpanded = state.displayMode === 'full' || isUserExpanded;
    const expandedClass = isUserExpanded ? ' paper--expanded' : '';
    const summaryBlock = summary
      ? `<p class="summary">${summary}</p>`
      : '';
    return `
      <article class="paper${expandedClass}" data-paper-id="${escapeHtml(articleId)}" tabindex="0" aria-expanded="${ariaExpanded}">
        <h3><a href="${absUrl}" target="_blank" rel="noopener">${title}</a>${keywordBadges}${quickViewButton}</h3>
        <p class="meta">
          <span class="id">${escapeHtml(article.arxiv_id)}</span>
          <span class="authors">${authors}</span>
        </p>
        ${summaryBlock}
        <p class="subjects">${subjects}</p>
        <p class="links">${linkItems}</p>
      </article>
    `;
  }

  function buildSectionGrouping(articles) {
    const sections = new Map();
    articles.forEach((article) => {
      const sectionKey = article.section_type || 'Other';
      const subjectKey = article.primary_subject || 'Other';
      if (!sections.has(sectionKey)) {
        sections.set(sectionKey, new Map());
      }
      const subjectMap = sections.get(sectionKey);
      if (!subjectMap.has(subjectKey)) {
        subjectMap.set(subjectKey, []);
      }
      subjectMap.get(subjectKey).push(article);
    });

    return Array.from(sections.entries())
      .sort(([a], [b]) => a.localeCompare(b))
      .map(([sectionName, subjectMap]) => {
        const sectionId = `category-${slugify(sectionName)}`;
        const subjects = Array.from(subjectMap.entries())
          .sort(([a], [b]) => a.localeCompare(b))
          .map(([subjectName, items]) => ({
            subjectId: `${sectionId}-${slugify(subjectName, 'subject')}`,
            subjectLabel: subjectName,
            items,
          }));
        const count = subjects.reduce((sum, entry) => sum + entry.items.length, 0);
        return {
          sectionId,
          sectionLabel: sectionName,
          count,
          subjects,
        };
      });
  }

  function filterByFavoriteAuthors(articles, favoriteAuthors) {
    const favorites = (favoriteAuthors || []).map((name) => name.toLowerCase()).filter(Boolean);
    if (!favorites.length) return [];
    return articles.filter((article) => {
      const authorLower = article.authors.map((name) => name.toLowerCase());
      return favorites.some((fav) => authorLower.some((author) => author.includes(fav)));
    });
  }

  function filterByKeywords(articles, keywords) {
    const needles = (keywords || []).map((kw) => kw.toLowerCase()).filter(Boolean);
    if (!needles.length) return [];
    return articles.filter((article) => {
      const articleKeywords = Array.isArray(article.keywords)
        ? article.keywords.map((kw) => kw.toLowerCase())
        : [];
      if (articleKeywords.length) {
        return needles.some((needle) => articleKeywords.includes(needle));
      }
      const fallback = `${article.title} ${article.abstract}`.toLowerCase();
      return needles.some((needle) => fallback.includes(needle));
    });
  }

  function pruneExpandedArticles(articles) {
    const ids = new Set(
      (articles || []).map((article) => String(article.arxiv_id || article.id || '')).filter((value) => value),
    );
    Array.from(state.expandedArticles).forEach((storedId) => {
      if (!ids.has(storedId)) {
        state.expandedArticles.delete(storedId);
      }
    });
  }

  function updatePreferenceInputs() {
    if (elements.favoritesInput) {
      elements.favoritesInput.value = state.preferences.favorite_authors.join('\n');
    }
    if (elements.keywordsInput) {
      elements.keywordsInput.value = state.preferences.keywords.join('\n');
    }
  }

  function setStatus(message) {
    if (elements.preferencesStatus) {
      elements.preferencesStatus.textContent = state.isEditingPreferences ? message : '';
    }
    if (elements.preferencesStatusView) {
      elements.preferencesStatusView.textContent = state.isEditingPreferences ? '' : message;
    }
  }

  function escapeHtml(value) {
    return String(value || '')
      .replace(/&/g, '&amp;')
      .replace(/</g, '&lt;')
      .replace(/>/g, '&gt;')
      .replace(/"/g, '&quot;')
      .replace(/'/g, '&#39;');
  }

  function slugify(text, fallback = 'section') {
    const slug = String(text || '')
      .trim()
      .toLowerCase()
      .replace(/[^a-z0-9]+/g, '-')
      .replace(/^-+|-+$/g, '');
    return slug || fallback;
  }

  function formatCount(value) {
    const count = Number(value) || 0;
    return `${count} paper${count === 1 ? '' : 's'}`;
  }

  function normalizeDisplayMode(value) {
    if (typeof value !== 'string') {
      return 'full';
    }
    const normalized = value.trim().toLowerCase();
    return Object.prototype.hasOwnProperty.call(DISPLAY_MODE_CLASSES, normalized) ? normalized : 'full';
  }

  function normalizePreferences(raw) {
    const normalizeList = (value) => {
      if (Array.isArray(value)) {
        const cleaned = value.map((item) => String(item).trim()).filter((item) => item);
        return Array.from(new Set(cleaned));
      }
      if (typeof value === 'string') {
        const cleaned = value
          .split(/[\n,]/)
          .map((item) => item.trim())
          .filter((item) => item);
        return Array.from(new Set(cleaned));
      }
      return [];
    };
    return {
      favorite_authors: normalizeList(raw.favorite_authors),
      keywords: normalizeList(raw.keywords),
    };
  }

  function loadStoredDisplayMode() {
    try {
      const stored = localStorage.getItem(DISPLAY_MODE_STORAGE_KEY);
      return stored ? normalizeDisplayMode(stored) : '';
    } catch (_) {
      return '';
    }
  }

  function saveDisplayMode(mode) {
    try {
      localStorage.setItem(DISPLAY_MODE_STORAGE_KEY, normalizeDisplayMode(mode));
    } catch (_) {}
  }

  function loadStoredSource() {
    try {
      return localStorage.getItem(SOURCE_STORAGE_KEY) || '';
    } catch (_) {
      return '';
    }
  }

  function saveSource(value) {
    try {
      localStorage.setItem(SOURCE_STORAGE_KEY, value);
    } catch (_) {}
  }

  function loadStoredPreferences() {
    try {
      const raw = localStorage.getItem(PREF_STORAGE_KEY);
      return raw ? normalizePreferences(JSON.parse(raw)) : normalizePreferences(initialPreferences);
    } catch (_) {
      return normalizePreferences(initialPreferences);
    }
  }

  function savePreferences(prefs) {
    try {
      localStorage.setItem(PREF_STORAGE_KEY, JSON.stringify(prefs));
    } catch (_) {}
  }

  function decodeHtml(value) {
    if (!value) return '';
    const textarea = document.createElement('textarea');
    textarea.innerHTML = value;
    return textarea.value;
  }

  function buildModalDetails(articleId, button) {
    const sourceData = RAW_DATA.sources[state.source] || {};
    const articles = Array.isArray(sourceData.articles) ? sourceData.articles : [];
    const article = articleId ? articles.find((item) => {
      const id = String(item.arxiv_id || item.id || '');
      return id === articleId;
    }) : null;
    const dataset = button.dataset || {};
    const details = {
      title: article && article.title ? article.title : dataset.articleTitle || '',
      url: article && article.abs_url ? article.abs_url : dataset.absUrl || '',
      authors: Array.isArray(article && article.authors) ? article.authors.join(', ') : dataset.articleAuthors || '',
      subjects: Array.isArray(article && article.subjects) ? article.subjects.join('; ') : dataset.articleSubjects || '',
      abstract: article && article.abstract ? article.abstract : dataset.articleAbstract || '',
      summary: article && article.summary ? article.summary : dataset.articleSummary || '',
      arxivId: article && article.arxiv_id ? article.arxiv_id : dataset.articleId || '',
      pdfUrl: article && article.pdf_url ? article.pdf_url : dataset.articlePdf || '',
    };
    return details;
  }
})();

  

  </script>
</body>
</html>
